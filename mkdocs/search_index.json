{
    "docs": [
        {
            "location": "/", 
            "text": "This is Maxwell's daemon, an application that reads MySQL binlogs and writes\nrow updates as JSON to Kafka, Kinesis, or other streaming platforms.  Maxwell has\nlow operational overhead, requiring nothing but mysql and a place to write to.\nIts common use cases include ETL, cache building/expiring, metrics collection,\nsearch indexing and inter-service communication.  Maxwell gives you some of the\nbenefits of event sourcing without having to re-architect your entire platform.\n\n\nDownload:\n\n\nhttps://github.com/zendesk/maxwell/releases/download/v1.22.1/maxwell-1.22.1.tar.gz\n\n\n\n\nSource:\n\n\nhttps://github.com/zendesk/maxwell\n\n\n\n\n  mysql\n insert into `test`.`maxwell` set id = 1, daemon = 'Stanislaw Lem';\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \ninsert\n,\n    \nts\n: 1449786310,\n    \nxid\n: 940752,\n    \ncommit\n: true,\n    \ndata\n: { \nid\n:1, \ndaemon\n: \nStanislaw Lem\n }\n  }\n\n\n\n\n  mysql\n update test.maxwell set daemon = 'firebus!  firebus!' where id = 1;\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \nupdate\n,\n    \nts\n: 1449786341,\n    \nxid\n: 940786,\n    \ncommit\n: true,\n    \ndata\n: {\nid\n:1, \ndaemon\n: \nFirebus!  Firebus!\n},\n    \nold\n:  {\ndaemon\n: \nStanislaw Lem\n}\n  }", 
            "title": "Overview"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Download\n\n\n\n\n\n\nDownload binary distro: \nhttps://github.com/zendesk/maxwell/releases/download/v1.22.1/maxwell-1.22.1.tar.gz\n\n\nSources and bug tracking is available on github: \nhttps://github.com/zendesk/maxwell\n\n\nObligatory copy/paste to terminal:\n\n\n\n\ncurl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.22.1/maxwell-1.22.1.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.22.1\n\n\n\n\nor get the docker image:\n\n\ndocker pull zendesk/maxwell\n\n\n\n\nor on Mac OS X with homebrew installed:\n\n\nbrew install maxwell\n\n\n\n\nConfigure Mysql\n\n\n\n\nServer Config:\n Ensure your server_id is configured, and that row-based replication is turned on.\n\n\n$ vi my.cnf\n\n[mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row\n\n\n\n\nOr on a running server:\n\n\nmysql\n set global binlog_format=ROW;\nmysql\n set global binlog_row_image=FULL;\n\n\n\n\nnote: \nbinlog_format\n is a session-based property.  You will need to shutdown all active connections to fully convert\nto row-based replication.\n\n\nPermissions:\n Maxwell needs permissions to store state in the database specified by the \nschema_database\n option (default \nmaxwell\n).\n\n\nmysql\n CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX';\nmysql\n GRANT ALL ON maxwell.* TO 'maxwell'@'%';\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql\n CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX';\nmysql\n GRANT ALL ON maxwell.* TO 'maxwell'@'localhost';\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost';\n\n\n\n\nRun Maxwell\n\n\n\n\nCommand line\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout\n\n\n\n\nDocker\n\n\ndocker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\\n    --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout\n\n\n\n\nKafka\n\n\nBoot kafka as described here:  \nhttp://kafka.apache.org/documentation.html#quickstart\n, then:\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell\n\n\n\n\n(or docker):\n\n\ndocker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\\n    --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\\n    --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell\n\n\n\n\nKinesis\n\n\ndocker run -it --rm --name maxwell -v `cd \n pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties \n echo \nRegion=$AWS_DEFAULT_REGION\n \n /app/kinesis-producer-library.properties \n bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM'\n\n\n\n\nGoogle Cloud Pub/Sub\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n  --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\\n  --pubsub_topic='maxwell'\n\n\n\n\nRabbitMQ\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n    --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname'\n\n\n\n\nRedis\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n    --producer=redis --redis_host=redis.hostname", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quickstart/#download", 
            "text": "Download binary distro:  https://github.com/zendesk/maxwell/releases/download/v1.22.1/maxwell-1.22.1.tar.gz  Sources and bug tracking is available on github:  https://github.com/zendesk/maxwell  Obligatory copy/paste to terminal:   curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.22.1/maxwell-1.22.1.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.22.1  or get the docker image:  docker pull zendesk/maxwell  or on Mac OS X with homebrew installed:  brew install maxwell", 
            "title": "Download"
        }, 
        {
            "location": "/quickstart/#configure-mysql", 
            "text": "Server Config:  Ensure your server_id is configured, and that row-based replication is turned on.  $ vi my.cnf\n\n[mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row  Or on a running server:  mysql  set global binlog_format=ROW;\nmysql  set global binlog_row_image=FULL;  note:  binlog_format  is a session-based property.  You will need to shutdown all active connections to fully convert\nto row-based replication.  Permissions:  Maxwell needs permissions to store state in the database specified by the  schema_database  option (default  maxwell ).  mysql  CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX';\nmysql  GRANT ALL ON maxwell.* TO 'maxwell'@'%';\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql  CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX';\nmysql  GRANT ALL ON maxwell.* TO 'maxwell'@'localhost';\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost';", 
            "title": "Configure Mysql"
        }, 
        {
            "location": "/quickstart/#run-maxwell", 
            "text": "", 
            "title": "Run Maxwell"
        }, 
        {
            "location": "/quickstart/#command-line", 
            "text": "bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout", 
            "title": "Command line"
        }, 
        {
            "location": "/quickstart/#docker", 
            "text": "docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\\n    --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout", 
            "title": "Docker"
        }, 
        {
            "location": "/quickstart/#kafka", 
            "text": "Boot kafka as described here:   http://kafka.apache.org/documentation.html#quickstart , then:  bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell  (or docker):  docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\\n    --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\\n    --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell", 
            "title": "Kafka"
        }, 
        {
            "location": "/quickstart/#kinesis", 
            "text": "docker run -it --rm --name maxwell -v `cd   pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties   echo  Region=$AWS_DEFAULT_REGION    /app/kinesis-producer-library.properties   bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM'", 
            "title": "Kinesis"
        }, 
        {
            "location": "/quickstart/#google-cloud-pubsub", 
            "text": "bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n  --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\\n  --pubsub_topic='maxwell'", 
            "title": "Google Cloud Pub/Sub"
        }, 
        {
            "location": "/quickstart/#rabbitmq", 
            "text": "bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n    --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname'", 
            "title": "RabbitMQ"
        }, 
        {
            "location": "/quickstart/#redis", 
            "text": "bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n    --producer=redis --redis_host=redis.hostname", 
            "title": "Redis"
        }, 
        {
            "location": "/config/", 
            "text": "Reference\n\n\n\n\nAt the minimum, you will need to specify 'host', 'user', 'password', 'producer'.\nThe kafka producer requires 'kafka.bootstrap.servers', the kinesis producer requires 'kinesis_stream'.\n\n\n\n\n\n\n\n\noption\n\n\nargument\n\n\ndescription\n\n\ndefault\n\n\n\n\n\n\n\n\n\n\ngeneral options\n\n\n\n\n\n\n\n\n\n\n\n\nconfig\n\n\nSTRING\n\n\nlocation of \nconfig.properties\n file\n\n\n$PWD/config.properties\n\n\n\n\n\n\nlog_level\n\n\n[debug \n info \n warn \n error]\n\n\nlog level\n\n\ninfo\n\n\n\n\n\n\ndaemon\n\n\n\n\nrunning maxwell as a daemon\n\n\n\n\n\n\n\n\nenv_config_prefix\n\n\nSTRING\n\n\nenv vars matching prefix are treated as config values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmysql options\n\n\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nSTRING\n\n\nmysql host\n\n\nlocalhost\n\n\n\n\n\n\nuser\n\n\nSTRING\n\n\nmysql username\n\n\n\n\n\n\n\n\npassword\n\n\nSTRING\n\n\nmysql password\n\n\n(no password)\n\n\n\n\n\n\nport\n\n\nINT\n\n\nmysql port\n\n\n3306\n\n\n\n\n\n\njdbc_options\n\n\nSTRING\n\n\nmysql jdbc connection options\n\n\nDEFAULT_JDBC_OPTS\n\n\n\n\n\n\nssl\n\n\nSSL_OPT\n\n\nSSL behavior for mysql cx\n\n\nDISABLED\n\n\n\n\n\n\nschema_database\n\n\nSTRING\n\n\ndatabase to store schema and position in\n\n\nmaxwell\n\n\n\n\n\n\nclient_id\n\n\nSTRING\n\n\nunique text identifier for maxwell instance\n\n\nmaxwell\n\n\n\n\n\n\nreplica_server_id\n\n\nLONG\n\n\nunique numeric identifier for this maxwell instance\n\n\n6379 (see \nnotes\n)\n\n\n\n\n\n\nmaster_recovery\n\n\nBOOLEAN\n\n\nenable experimental master recovery code\n\n\nfalse\n\n\n\n\n\n\ngtid_mode\n\n\nBOOLEAN\n\n\nenable GTID-based replication\n\n\nfalse\n\n\n\n\n\n\nrecapture_schema\n\n\nBOOLEAN\n\n\nrecapture the latest schema. Not available in config.properties.\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreplication_host\n\n\nSTRING\n\n\nserver to replicate from.  See \nsplit server roles\n\n\nschema-store host\n\n\n\n\n\n\nreplication_password\n\n\nSTRING\n\n\npassword on replication server\n\n\n(none)\n\n\n\n\n\n\nreplication_port\n\n\nINT\n\n\nport on replication server\n\n\n3306\n\n\n\n\n\n\nreplication_user\n\n\nSTRING\n\n\nuser on replication server\n\n\n\n\n\n\n\n\nreplication_ssl\n\n\nSSL_OPT\n\n\nSSL behavior for replication cx cx\n\n\nDISABLED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nschema_host\n\n\nSTRING\n\n\nserver to capture schema from.  See \nsplit server roles\n\n\nschema-store host\n\n\n\n\n\n\nschema_password\n\n\nSTRING\n\n\npassword on schema-capture server\n\n\n(none)\n\n\n\n\n\n\nschema_port\n\n\nINT\n\n\nport on schema-capture server\n\n\n3306\n\n\n\n\n\n\nschema_user\n\n\nSTRING\n\n\nuser on schema-capture server\n\n\n\n\n\n\n\n\nschema_ssl\n\n\nSSL_OPT\n\n\nSSL behavior for schema-capture server\n\n\nDISABLED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproducer options\n\n\n\n\n\n\n\n\n\n\n\n\nproducer\n\n\nPRODUCER_TYPE\n\n\ntype of producer to use\n\n\nstdout\n\n\n\n\n\n\ncustom_producer.factory\n\n\nCLASS_NAME\n\n\nfully qualified custom producer factory class, see \nexample\n\n\n\n\n\n\n\n\nproducer_ack_timeout\n\n\nPRODUCER_ACK_TIMEOUT\n\n\ntime in milliseconds before async producers consider a message lost\n\n\n\n\n\n\n\n\nproducer_partition_by\n\n\nPARTITION_BY\n\n\ninput to kafka/kinesis partition function\n\n\ndatabase\n\n\n\n\n\n\nproducer_partition_columns\n\n\nSTRING\n\n\nif partitioning by 'column', a comma separated list of columns\n\n\n\n\n\n\n\n\nproducer_partition_by_fallback\n\n\nPARTITION_BY_FALLBACK\n\n\nrequired when producer_partition_by=column.  Used when the column is missing\n\n\n\n\n\n\n\n\nignore_producer_error\n\n\nBOOLEAN\n\n\nWhen false, Maxwell will terminate on kafka/kinesis publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"file\" producer options\n\n\n\n\n\n\n\n\n\n\n\n\noutput_file\n\n\nSTRING\n\n\noutput file for \nfile\n producer\n\n\n\n\n\n\n\n\njavascript\n\n\nSTRING\n\n\nfile containing javascript filters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"kafka\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\nkafka.bootstrap.servers\n\n\nSTRING\n\n\nkafka brokers, given as \nHOST:PORT[,HOST:PORT]\n\n\n\n\n\n\n\n\nkafka_topic\n\n\nSTRING\n\n\nkafka topic to write to.\n\n\nmaxwell\n\n\n\n\n\n\ndead_letter_topic\n\n\nSTRING\n\n\nthe topic to write a \"skeleton row\" (a row where \ndata\n includes only primary key columns) when there's an error publishing a row. When \nignore_producer_error\n is \nfalse\n, only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher\n\n\n\n\n\n\n\n\nkafka_version\n\n\nKAFKA_VERSION\n\n\nrun maxwell with specified kafka producer version.  Not available in config.properties.\n\n\n0.11.0.1\n\n\n\n\n\n\nkafka_partition_hash\n\n\n[ default \n murmur3 ]\n\n\nhash function to use when choosing kafka partition\n\n\ndefault\n\n\n\n\n\n\nkafka_key_format\n\n\n[ array \n hash ]\n\n\nhow maxwell outputs kafka keys, either a hash or an array of hashes\n\n\nhash\n\n\n\n\n\n\nddl_kafka_topic\n\n\nSTRING\n\n\nif output_ddl is true, kafka topic to write DDL changes to\n\n\nkafka_topic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"kinesis\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\nkinesis_stream\n\n\nSTRING\n\n\nkinesis stream name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"sqs\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\nsqs_queue_uri\n\n\nSTRING\n\n\nSQS Queue URI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"pubsub\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\npubsub_topic\n\n\nSTRING\n\n\nGoogle Cloud pub-sub topic\n\n\n\n\n\n\n\n\npubsub_platform_id\n\n\nSTRING\n\n\nGoogle Cloud platform id associated with topic\n\n\n\n\n\n\n\n\nddl_pubsub_topic\n\n\nSTRING\n\n\nGoogle Cloud pub-sub topic to send DDL events to\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"rabbitmq\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\nrabbitmq_user\n\n\nSTRING\n\n\nUsername of Rabbitmq connection\n\n\nguest\n\n\n\n\n\n\nrabbitmq_pass\n\n\nSTRING\n\n\nPassword of Rabbitmq connection\n\n\nguest\n\n\n\n\n\n\nrabbitmq_host\n\n\nSTRING\n\n\nHost of Rabbitmq machine\n\n\n\n\n\n\n\n\nrabbitmq_port\n\n\nINT\n\n\nPort of Rabbitmq machine\n\n\n\n\n\n\n\n\nrabbitmq_virtual_host\n\n\nSTRING\n\n\nVirtual Host of Rabbitmq\n\n\n\n\n\n\n\n\nrabbitmq_exchange\n\n\nSTRING\n\n\nName of exchange for rabbitmq publisher\n\n\n\n\n\n\n\n\nrabbitmq_exchange_type\n\n\nSTRING\n\n\nExchange type for rabbitmq\n\n\n\n\n\n\n\n\nrabbitmq_exchange_durable\n\n\nBOOLEAN\n\n\nExchange durability.\n\n\nfalse\n\n\n\n\n\n\nrabbitmq_exchange_autodelete\n\n\nBOOLEAN\n\n\nIf set, the exchange is deleted when all queues have finished using it.\n\n\nfalse\n\n\n\n\n\n\nrabbitmq_routing_key_template\n\n\nSTRING\n\n\nA string template for the routing key, \n%db%\n and \n%table%\n will be substituted.\n\n\n%db%.%table%\n.\n\n\n\n\n\n\nrabbitmq_message_persistent\n\n\nBOOLEAN\n\n\nEanble message persistence.\n\n\nfalse\n\n\n\n\n\n\nrabbitmq_declare_exchange\n\n\nBOOLEAN\n\n\nShould declare the exchange for rabbitmq publisher\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"redis\" producer options \n\n\n\n\n\n\n\n\n\n\n\n\nredis_host\n\n\nSTRING\n\n\nHost of Redis server\n\n\nlocalhost\n\n\n\n\n\n\nredis_port\n\n\nINT\n\n\nPort of Redis server\n\n\n6379\n\n\n\n\n\n\nredis_auth\n\n\nSTRING\n\n\nAuthentication key for a password-protected Redis server\n\n\n\n\n\n\n\n\nredis_database\n\n\nINT\n\n\nDatabase of Redis server\n\n\n0\n\n\n\n\n\n\nredis_pub_channel\n\n\nSTRING\n\n\nRedis Pub/Sub channel\n\n\nmaxwell\n\n\n\n\n\n\nredis_list_key\n\n\nSTRING\n\n\nRedis LPUSH List Key\n\n\nmaxwell\n\n\n\n\n\n\nredis_type\n\n\n[ pubsub \n lpush ]\n\n\nSelects either Redis Pub/Sub or LPUSH.\n\n\npubsub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nformatting\n\n\n\n\n\n\n\n\n\n\n\n\noutput_binlog_position\n\n\nBOOLEAN\n\n\nrecords include binlog position\n\n\nfalse\n\n\n\n\n\n\noutput_gtid_position\n\n\nBOOLEAN\n\n\nrecords include gtid position, if available\n\n\nfalse\n\n\n\n\n\n\noutput_commit_info\n\n\nBOOLEAN\n\n\nrecords include commit and xid\n\n\ntrue\n\n\n\n\n\n\noutput_xoffset\n\n\nBOOLEAN\n\n\nrecords include virtual tx-row offset\n\n\nfalse\n\n\n\n\n\n\noutput_nulls\n\n\nBOOLEAN\n\n\nrecords include fields with NULL values\n\n\ntrue\n\n\n\n\n\n\noutput_server_id\n\n\nBOOLEAN\n\n\nrecords include server_id\n\n\nfalse\n\n\n\n\n\n\noutput_thread_id\n\n\nBOOLEAN\n\n\nrecords include thread_id\n\n\nfalse\n\n\n\n\n\n\noutput_schema_id\n\n\nBOOLEAN\n\n\nrecords include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value\n\n\nfalse\n\n\n\n\n\n\noutput_row_query\n\n\nBOOLEAN\n\n\nrecords include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled\n\n\nfalse\n\n\n\n\n\n\noutput_ddl\n\n\nBOOLEAN\n\n\noutput DDL (table-alter, table-create, etc) events\n\n\nfalse\n\n\n\n\n\n\noutput_null_zerodates\n\n\nBOOLEAN\n\n\nshould we transform '0000-00-00' to null?\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltering\n\n\n\n\n\n\n\n\n\n\n\n\nfilter\n\n\nSTRING\n\n\nfilter rules, eg \nexclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencryption\n\n\n\n\n\n\n\n\n\n\n\n\nencrypt\n\n\n[ none \n data \n all ]\n\n\nencrypt mode: none = no encryption. \"data\": encrypt the \ndata\n field only. \nall\n: encrypt entire maxwell message\n\n\nnone\n\n\n\n\n\n\nsecret_key\n\n\nSTRING\n\n\nspecify the encryption key to be used\n\n\nnull\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonitoring / metrics\n\n\n\n\n\n\n\n\n\n\n\n\nmetrics_prefix\n\n\nSTRING\n\n\nthe prefix maxwell will apply to all metrics\n\n\nMaxwellMetrics\n\n\n\n\n\n\nmetrics_type\n\n\n[slf4j \n jmx \n http \n datadog]\n\n\nhow maxwell metrics will be reported\n\n\n\n\n\n\n\n\nmetrics_jvm\n\n\nBOOLEAN\n\n\nenable jvm metrics: memory usage, GC stats, etc.\n\n\nfalse\n\n\n\n\n\n\nmetrics_slf4j_interval\n\n\nSECONDS\n\n\nthe frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured\n\n\n60\n\n\n\n\n\n\nhttp_port\n\n\nINT\n\n\nthe port the server will bind to when http reporting is configured\n\n\n8080\n\n\n\n\n\n\nhttp_path_prefix\n\n\nSTRING\n\n\nhttp path prefix for the server\n\n\n/\n\n\n\n\n\n\nhttp_bind_address\n\n\nSTRING\n\n\nthe address the server will bind to when http reporting is configured\n\n\nall addresses\n\n\n\n\n\n\nhttp_diagnostic\n\n\nBOOLEAN\n\n\nenable http diagnostic endpoint\n\n\nfalse\n\n\n\n\n\n\nhttp_diagnostic_timeout\n\n\nMILLISECONDS\n\n\nthe http diagnostic response timeout\n\n\n10000\n\n\n\n\n\n\nmetrics_datadog_type\n\n\n[udp \n http]\n\n\nwhen metrics_type includes \ndatadog\n this is the way metrics will be reported, can only be one of [udp \n http]\n\n\nudp\n\n\n\n\n\n\nmetrics_datadog_tags\n\n\nSTRING\n\n\ndatadog tags that should be supplied, e.g. tag1:value1,tag2:value2\n\n\n\n\n\n\n\n\nmetrics_datadog_interval\n\n\nINT\n\n\nthe frequency metrics are pushed to datadog, in seconds\n\n\n60\n\n\n\n\n\n\nmetrics_datadog_apikey\n\n\nSTRING\n\n\nthe datadog api key to use when metrics_datadog_type = \nhttp\n\n\n\n\n\n\n\n\nmetrics_datadog_host\n\n\nSTRING\n\n\nthe host to publish metrics to when metrics_datadog_type = \nudp\n\n\nlocalhost\n\n\n\n\n\n\nmetrics_datadog_port\n\n\nINT\n\n\nthe port to publish metrics to when metrics_datadog_type = \nudp\n\n\n8125\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\n\nbootstrapper\n\n\n[async \n sync \n none]\n\n\nbootstrapper type.  See bootstrapping docs.\n\n\nasync\n\n\n\n\n\n\ninit_position\n\n\nFILE:POSITION[:HEARTBEAT]\n\n\nignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.\n\n\n\n\n\n\n\n\nreplay\n\n\nBOOLEAN\n\n\nenable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes.  Not available in config.properties.\n\n\n\n\n\n\n\n\n\n\n\nSSL_OPTION: [ DISABLED \n PREFERRED \n REQUIRED \n VERIFY_CA \n VERIFY_IDENTITY ]\n\n\n\n\n\nPRODUCER_TYPE: [ stdout \n file \n kafka \n kinesis \n pubsub \n sqs \n rabbitmq \n redis ]\n\n\n\n\n\nDEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull\nconnectTimeout=5000\n\n\n\n\n\nPARTITION_BY: [ database \n table \n primary_key \n transaction_id \n column ]\n\n\n\n\n\nPARTITION_BY_FALLBACK: [ database \n table \n primary_key \n transaction_id ]\n\n\n\n\n\nKAFKA_VERSION: [ 0.8.2.2 \n 0.9.0.1 \n 0.10.0.1 \n 0.10.2.1 \n 0.11.0.1 ]\n\n\n\n\n\nPRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear\na message, never notifying maxwell of success or failure.  This timeout can be set as a heuristic; after this many\nmilliseconds, maxwell will consider an outstanding message lost and fail it.\n\n\n\n\nConfiguration methods\n\n\n\n\nMaxwell is configurable via the command-line, a properties file, or the environment.\nThe configuration priority is:\n\n\ncommand line options \n scoped env vars \n properties file \n default values\n\n\n\n\nconfig.properties\n\n\nMaxwell can be configured via a java properties file, specified via \n--config\n\nor named \"config.properties\" in the current working directory.\nAny command line options (except \ninit_position\n, \nreplay\n, \nkafka_version\n and\n\ndaemon\n) may be specified as \"key=value\" pairs.\n\n\nvia environment\n\n\nIf \nenv_config_prefix\n given via command line or in \nconfig.properties\n, Maxwell\nwill configure itself with all environment variables that match the prefix. The\nenvironment variable names are case insensitive.  For example, if maxwell is\nstarted with \n--env_config_prefix=FOO_\n and the environment contains \nFOO_USER=auser\n,\nthis would be equivalent to passing \n--user=auser\n.\n\n\nDeployment scenarios\n\n\n\n\nAt a minimum, Maxwell needs row-level-replication turned on into order to\noperate:\n\n\n[mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row\n\n\n\n\nGTID support\n\n\nAs of 1.8.0, Maxwell contains support for\n\nGTID-based replication\n.\nEnable it with the \n--gtid_mode\n configuration param.\n\n\nHere's how you might configure your mysql server for GTID mode:\n\n\n$ vi my.cnf\n\n[mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row\ngtid-mode=ON\nlog-slave-updates=ON\nenforce-gtid-consistency=true\n\n\n\n\nWhen in GTID-mode, Maxwell will transparently pick up a new replication\nposition after a master change.  Note that you will still have to re-point\nmaxwell to the new master.\n\n\nGTID support in Maxwell is considered beta-quality at the moment; notably,\nMaxwell is unable to transparently upgrade from a traditional-replication\nscenario to a GTID-replication scenario; currently, when you enable gtid mode\nMaxwell will recapture the schema and GTID-position from \"wherever the master\nis at\".\n\n\nRDS configuration\n\n\nTo run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:\n\n\n\n\nset binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".\n\n\nsetup RDS binlog retention as described \nhere\n.\n  The tl;dr is to execute \ncall mysql.rds_set_configuration('binlog retention hours', 24)\n on the server.\n\n\n\n\nSplit server roles\n\n\nMaxwell uses MySQL for 3 different functions:\n\n\n\n\nA host to store the captured schema in (\n--host\n).\n\n\nA host to replicate from (\n--replication_host\n).\n\n\nA host to capture the schema from (\n--schema_host\n).\n\n\n\n\nOften, all three hosts are the same.  \nhost\n and \nreplication_host\n should be different\nif maxwell is chained off a replica.  \nschema_host\n should only be used when using the\nmaxscale replication proxy.\n\n\nMultiple Maxwell Instances\n\n\nMaxwell can operate with multiple instances running against a single master, in\ndifferent configurations.  This can be useful if you wish to have producers\nrunning in different configurations, for example producing different groups of\ntables to different topics.  Each instance of Maxwell must be configured with a\nunique \nclient_id\n, in order to store unique binlog positions.\n\n\nWith MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must\nalso be configured with a unique \nreplica_server_id\n.  This is a 32-bit integer\nthat corresponds to mysql's \nserver_id\n parameter.  The value you configure\nshould be unique across all mysql and maxwell instances.", 
            "title": "Configuration"
        }, 
        {
            "location": "/config/#reference", 
            "text": "At the minimum, you will need to specify 'host', 'user', 'password', 'producer'.\nThe kafka producer requires 'kafka.bootstrap.servers', the kinesis producer requires 'kinesis_stream'.     option  argument  description  default      general options       config  STRING  location of  config.properties  file  $PWD/config.properties    log_level  [debug   info   warn   error]  log level  info    daemon   running maxwell as a daemon     env_config_prefix  STRING  env vars matching prefix are treated as config values           mysql options       host  STRING  mysql host  localhost    user  STRING  mysql username     password  STRING  mysql password  (no password)    port  INT  mysql port  3306    jdbc_options  STRING  mysql jdbc connection options  DEFAULT_JDBC_OPTS    ssl  SSL_OPT  SSL behavior for mysql cx  DISABLED    schema_database  STRING  database to store schema and position in  maxwell    client_id  STRING  unique text identifier for maxwell instance  maxwell    replica_server_id  LONG  unique numeric identifier for this maxwell instance  6379 (see  notes )    master_recovery  BOOLEAN  enable experimental master recovery code  false    gtid_mode  BOOLEAN  enable GTID-based replication  false    recapture_schema  BOOLEAN  recapture the latest schema. Not available in config.properties.  false          replication_host  STRING  server to replicate from.  See  split server roles  schema-store host    replication_password  STRING  password on replication server  (none)    replication_port  INT  port on replication server  3306    replication_user  STRING  user on replication server     replication_ssl  SSL_OPT  SSL behavior for replication cx cx  DISABLED          schema_host  STRING  server to capture schema from.  See  split server roles  schema-store host    schema_password  STRING  password on schema-capture server  (none)    schema_port  INT  port on schema-capture server  3306    schema_user  STRING  user on schema-capture server     schema_ssl  SSL_OPT  SSL behavior for schema-capture server  DISABLED          producer options       producer  PRODUCER_TYPE  type of producer to use  stdout    custom_producer.factory  CLASS_NAME  fully qualified custom producer factory class, see  example     producer_ack_timeout  PRODUCER_ACK_TIMEOUT  time in milliseconds before async producers consider a message lost     producer_partition_by  PARTITION_BY  input to kafka/kinesis partition function  database    producer_partition_columns  STRING  if partitioning by 'column', a comma separated list of columns     producer_partition_by_fallback  PARTITION_BY_FALLBACK  required when producer_partition_by=column.  Used when the column is missing     ignore_producer_error  BOOLEAN  When false, Maxwell will terminate on kafka/kinesis publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic  true          \"file\" producer options       output_file  STRING  output file for  file  producer     javascript  STRING  file containing javascript filters           \"kafka\" producer options        kafka.bootstrap.servers  STRING  kafka brokers, given as  HOST:PORT[,HOST:PORT]     kafka_topic  STRING  kafka topic to write to.  maxwell    dead_letter_topic  STRING  the topic to write a \"skeleton row\" (a row where  data  includes only primary key columns) when there's an error publishing a row. When  ignore_producer_error  is  false , only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher     kafka_version  KAFKA_VERSION  run maxwell with specified kafka producer version.  Not available in config.properties.  0.11.0.1    kafka_partition_hash  [ default   murmur3 ]  hash function to use when choosing kafka partition  default    kafka_key_format  [ array   hash ]  how maxwell outputs kafka keys, either a hash or an array of hashes  hash    ddl_kafka_topic  STRING  if output_ddl is true, kafka topic to write DDL changes to  kafka_topic          \"kinesis\" producer options        kinesis_stream  STRING  kinesis stream name           \"sqs\" producer options        sqs_queue_uri  STRING  SQS Queue URI           \"pubsub\" producer options        pubsub_topic  STRING  Google Cloud pub-sub topic     pubsub_platform_id  STRING  Google Cloud platform id associated with topic     ddl_pubsub_topic  STRING  Google Cloud pub-sub topic to send DDL events to           \"rabbitmq\" producer options        rabbitmq_user  STRING  Username of Rabbitmq connection  guest    rabbitmq_pass  STRING  Password of Rabbitmq connection  guest    rabbitmq_host  STRING  Host of Rabbitmq machine     rabbitmq_port  INT  Port of Rabbitmq machine     rabbitmq_virtual_host  STRING  Virtual Host of Rabbitmq     rabbitmq_exchange  STRING  Name of exchange for rabbitmq publisher     rabbitmq_exchange_type  STRING  Exchange type for rabbitmq     rabbitmq_exchange_durable  BOOLEAN  Exchange durability.  false    rabbitmq_exchange_autodelete  BOOLEAN  If set, the exchange is deleted when all queues have finished using it.  false    rabbitmq_routing_key_template  STRING  A string template for the routing key,  %db%  and  %table%  will be substituted.  %db%.%table% .    rabbitmq_message_persistent  BOOLEAN  Eanble message persistence.  false    rabbitmq_declare_exchange  BOOLEAN  Should declare the exchange for rabbitmq publisher  true          \"redis\" producer options        redis_host  STRING  Host of Redis server  localhost    redis_port  INT  Port of Redis server  6379    redis_auth  STRING  Authentication key for a password-protected Redis server     redis_database  INT  Database of Redis server  0    redis_pub_channel  STRING  Redis Pub/Sub channel  maxwell    redis_list_key  STRING  Redis LPUSH List Key  maxwell    redis_type  [ pubsub   lpush ]  Selects either Redis Pub/Sub or LPUSH.  pubsub          formatting       output_binlog_position  BOOLEAN  records include binlog position  false    output_gtid_position  BOOLEAN  records include gtid position, if available  false    output_commit_info  BOOLEAN  records include commit and xid  true    output_xoffset  BOOLEAN  records include virtual tx-row offset  false    output_nulls  BOOLEAN  records include fields with NULL values  true    output_server_id  BOOLEAN  records include server_id  false    output_thread_id  BOOLEAN  records include thread_id  false    output_schema_id  BOOLEAN  records include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value  false    output_row_query  BOOLEAN  records include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled  false    output_ddl  BOOLEAN  output DDL (table-alter, table-create, etc) events  false    output_null_zerodates  BOOLEAN  should we transform '0000-00-00' to null?  false          filtering       filter  STRING  filter rules, eg  exclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val           encryption       encrypt  [ none   data   all ]  encrypt mode: none = no encryption. \"data\": encrypt the  data  field only.  all : encrypt entire maxwell message  none    secret_key  STRING  specify the encryption key to be used  null          monitoring / metrics       metrics_prefix  STRING  the prefix maxwell will apply to all metrics  MaxwellMetrics    metrics_type  [slf4j   jmx   http   datadog]  how maxwell metrics will be reported     metrics_jvm  BOOLEAN  enable jvm metrics: memory usage, GC stats, etc.  false    metrics_slf4j_interval  SECONDS  the frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured  60    http_port  INT  the port the server will bind to when http reporting is configured  8080    http_path_prefix  STRING  http path prefix for the server  /    http_bind_address  STRING  the address the server will bind to when http reporting is configured  all addresses    http_diagnostic  BOOLEAN  enable http diagnostic endpoint  false    http_diagnostic_timeout  MILLISECONDS  the http diagnostic response timeout  10000    metrics_datadog_type  [udp   http]  when metrics_type includes  datadog  this is the way metrics will be reported, can only be one of [udp   http]  udp    metrics_datadog_tags  STRING  datadog tags that should be supplied, e.g. tag1:value1,tag2:value2     metrics_datadog_interval  INT  the frequency metrics are pushed to datadog, in seconds  60    metrics_datadog_apikey  STRING  the datadog api key to use when metrics_datadog_type =  http     metrics_datadog_host  STRING  the host to publish metrics to when metrics_datadog_type =  udp  localhost    metrics_datadog_port  INT  the port to publish metrics to when metrics_datadog_type =  udp  8125          misc       bootstrapper  [async   sync   none]  bootstrapper type.  See bootstrapping docs.  async    init_position  FILE:POSITION[:HEARTBEAT]  ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.     replay  BOOLEAN  enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes.  Not available in config.properties.      \nSSL_OPTION: [ DISABLED   PREFERRED   REQUIRED   VERIFY_CA   VERIFY_IDENTITY ]  \nPRODUCER_TYPE: [ stdout   file   kafka   kinesis   pubsub   sqs   rabbitmq   redis ]  \nDEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull connectTimeout=5000  \nPARTITION_BY: [ database   table   primary_key   transaction_id   column ]  \nPARTITION_BY_FALLBACK: [ database   table   primary_key   transaction_id ]  \nKAFKA_VERSION: [ 0.8.2.2   0.9.0.1   0.10.0.1   0.10.2.1   0.11.0.1 ]  \nPRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear\na message, never notifying maxwell of success or failure.  This timeout can be set as a heuristic; after this many\nmilliseconds, maxwell will consider an outstanding message lost and fail it.", 
            "title": "Reference"
        }, 
        {
            "location": "/config/#configuration-methods", 
            "text": "Maxwell is configurable via the command-line, a properties file, or the environment.\nThe configuration priority is:  command line options   scoped env vars   properties file   default values", 
            "title": "Configuration methods"
        }, 
        {
            "location": "/config/#configproperties", 
            "text": "Maxwell can be configured via a java properties file, specified via  --config \nor named \"config.properties\" in the current working directory.\nAny command line options (except  init_position ,  replay ,  kafka_version  and daemon ) may be specified as \"key=value\" pairs.", 
            "title": "config.properties"
        }, 
        {
            "location": "/config/#via-environment", 
            "text": "If  env_config_prefix  given via command line or in  config.properties , Maxwell\nwill configure itself with all environment variables that match the prefix. The\nenvironment variable names are case insensitive.  For example, if maxwell is\nstarted with  --env_config_prefix=FOO_  and the environment contains  FOO_USER=auser ,\nthis would be equivalent to passing  --user=auser .", 
            "title": "via environment"
        }, 
        {
            "location": "/config/#deployment-scenarios", 
            "text": "At a minimum, Maxwell needs row-level-replication turned on into order to\noperate:  [mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row", 
            "title": "Deployment scenarios"
        }, 
        {
            "location": "/config/#gtid-support", 
            "text": "As of 1.8.0, Maxwell contains support for GTID-based replication .\nEnable it with the  --gtid_mode  configuration param.  Here's how you might configure your mysql server for GTID mode:  $ vi my.cnf\n\n[mysqld]\nserver_id=1\nlog-bin=master\nbinlog_format=row\ngtid-mode=ON\nlog-slave-updates=ON\nenforce-gtid-consistency=true  When in GTID-mode, Maxwell will transparently pick up a new replication\nposition after a master change.  Note that you will still have to re-point\nmaxwell to the new master.  GTID support in Maxwell is considered beta-quality at the moment; notably,\nMaxwell is unable to transparently upgrade from a traditional-replication\nscenario to a GTID-replication scenario; currently, when you enable gtid mode\nMaxwell will recapture the schema and GTID-position from \"wherever the master\nis at\".", 
            "title": "GTID support"
        }, 
        {
            "location": "/config/#rds-configuration", 
            "text": "To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:   set binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".  setup RDS binlog retention as described  here .\n  The tl;dr is to execute  call mysql.rds_set_configuration('binlog retention hours', 24)  on the server.", 
            "title": "RDS configuration"
        }, 
        {
            "location": "/config/#split-server-roles", 
            "text": "Maxwell uses MySQL for 3 different functions:   A host to store the captured schema in ( --host ).  A host to replicate from ( --replication_host ).  A host to capture the schema from ( --schema_host ).   Often, all three hosts are the same.   host  and  replication_host  should be different\nif maxwell is chained off a replica.   schema_host  should only be used when using the\nmaxscale replication proxy.", 
            "title": "Split server roles"
        }, 
        {
            "location": "/config/#multiple-maxwell-instances", 
            "text": "Maxwell can operate with multiple instances running against a single master, in\ndifferent configurations.  This can be useful if you wish to have producers\nrunning in different configurations, for example producing different groups of\ntables to different topics.  Each instance of Maxwell must be configured with a\nunique  client_id , in order to store unique binlog positions.  With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must\nalso be configured with a unique  replica_server_id .  This is a 32-bit integer\nthat corresponds to mysql's  server_id  parameter.  The value you configure\nshould be unique across all mysql and maxwell instances.", 
            "title": "Multiple Maxwell Instances"
        }, 
        {
            "location": "/producers/", 
            "text": "Kafka\n\n\n\n\nTopic\n\n\nMaxwell writes to a kafka topic named \"maxwell\" by default. It can be static,\ne.g. 'maxwell', or dynamic, e.g. \nnamespace_%{database}_%{table}\n. In the\nlatter case 'database' and 'table' will be replaced with the values for the row\nbeing processed. This can be changed with the \nkafka_topic\n option.\n\n\nClient version\n\n\nBy default, maxwell uses the kafka 1.0.0 library. The \n--kafka_version\n flag\nlets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or\n0.11.0.1, 1.0.0.  This flag is only available on the command line.\n\n\n\n\nThe 0.8.2.2 client is only compatible with brokers running kafka 0.8.\n\n\nThe 0.10.0.x client is only compatible with brokers 0.10.0.x or later.\n\n\nMixing the 0.10 client with other versions can lead to serious performance impacts.\n  For More details, \nread about it here\n.\n\n\nThe 0.11.0 client can talk to version 0.10.0 or newer brokers.\n\n\nThe 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case:\n\n\n\n\nERROR Sender - Uncaught error in kafka producer I/O thread:\nSchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException\n\n\n\n\nExtended options\n\n\nAny options given to Maxwell that are prefixed with \nkafka.\n will be passed directly into the Kafka producer configuration\n(with \nkafka.\n stripped off).  We use the \"new producer\" configuration, as described here:\n\nhttp://kafka.apache.org/documentation.html#newproducerconfigs\n\n\nPerformant properties\n\n\nThese properties would give high throughput performance.\n\n\nkafka.acks = 1\nkafka.compression.type = snappy\nkafka.retries=0\n\n\n\n\nAt-least-once properties\n\n\nFor at-least-once delivery, you will want something more like:\n\n\nkafka.acks = all\nkafka.retries = 5 # or some larger number\n\n\n\n\nAnd you will also want to set \nmin.insync.replicas\n on Maxwell's output topic.\n\n\nKeys\n\n\nMaxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:\n\n\n{ \ndatabase\n:\ntest_tb\n,\ntable\n:\ntest_tbl\n,\npk.id\n:4,\npk.part2\n:\nhello\n}\n\n\n\n\nThis key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.\n\n\nThe JSON-hash based key format is tricky to regenerate in a stable fashion.  If you have an\napplication in which you need to parse and re-generate keys, it's advised you enable\n\n--kafka_key_format=array\n, which will generate kafka keys that can be parsed and re-output byte-for-byte.\n\n\nPartitioning\n\n\n\n\nBoth Kafka and AWS Kinesis support the notion of partitioned streams.\nPartitioning is controlled by \nproducer_partition_by\n, which gives you the\noption to split your stream by database, table, primary\nkey, or column data.  How you choose to partition will influence the consumers\nof the maxwell stream.  A good rule of thumb is to use the finest-grained\npartition scheme possible given serialization constraints.\n\n\nTo partition by column data, you must set both:\n\n\n\n\nproducer_partition_columns\n, a comma-separated list of column names, and\n\n\nproducer_partiton_by_fallback\n. This may be (\ndatabase\n, \ntable\n,\n  \nprimary_key\n), and will be used as a default value when the column does not\n  exist.\n\n\n\n\nWhen partitioning by column Maxwell will treat the values for the specified\ncolumns as strings, concatenate them and use that value to partition the data.\n\n\nKafka partitioning\n\n\nA binlog event's partition is determined by the selected hash function and hash string as follows\n\n\n  HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS\n\n\n\n\nThe HASH_FUNCTION is either java's \nhashCode\n or \nmurmurhash3\n. The default\nHASH_FUNCTION is \nhashCode\n. Murmurhash3 may be set with the\n\nkafka_partition_hash\n option. The seed value for the murmurhash function is\nhardcoded to 25342 in the MaxwellKafkaPartitioner class.\n\n\nThe HASH_STRING may be (\ndatabase\n, \ntable\n, \nprimary_key\n, \ntransaction_id\n, \ncolumn\n).  The\ndefault HASH_STRING is the \ndatabase\n. The partitioning field can be configured\nusing the \nproducer_partition_by\n option.\n\n\nMaxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:\n\n\nbin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2\n\n\n\n\nhttp://kafka.apache.org/documentation.html#quickstart\n\n\nKinesis\n\n\n\n\nAWS Credentials\n\n\nYou will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to:\n\n\n\n\n\"kinesis:PutRecord\"\n\n\n\"kinesis:PutRecords\"\n\n\n\"kinesis:DescribeStream\"\n\n\n\"cloudwatch:PutMetricData\"\n\n\n\n\nSee the \nAWS docs\n for the latest examples on which permissions are needed.\n\n\nThe producer uses the \nDefaultAWSCredentialsProviderChain\n class to gain aws credentials.\nSee the \nAWS docs\n on how to setup the IAM user with the Default Credential Provider Chain.\n\n\nOptions\n\n\nSet the output stream in \nconfig.properties\n by setting the \nkinesis_stream\n property.\n\n\nThe producer uses the \nKPL (Kinesis Producer Library)\n and uses the KPL built in configurations.\nCopy \nkinesis-producer-library.properties.example\n to \nkinesis-producer-library.properties\n and configure the properties file to your needs.\n\n\nYou are \nrequired\n to configure the region. For example:\n\n\n# set explicitly\nRegion=us-west-2\n# or set with an environment variable\nRegion=$AWS_DEFAULT_REGION\n\n\n\n\nBy default, the KPL implements \nrecord aggregation\n, which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the \nKCL (Kinesis Client Library)\n to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the \nAWS Kinesis Aggregation library\n), or disable record aggregation in your \nkinesis-producer-library.properties\n configuration.\n\n\nTo disable aggregation, add the following to your configuration:\n\n\nAggregationEnabled=false\n\n\n\n\nRemember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput.\n\n\nSQS\n\n\n\n\nAWS Credentials\n\n\nYou will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses \nDefaultAWSCredentialsProviderChain\n to get AWS credentials.\n\n\nSee the \nAWS docs\n on how to setup the IAM user with the Default Credential Provider Chain.\n\n\nIn case you need to set up a different region also along with credentials then default one, see the \nAWS docs\n.\n\n\nOptions\n\n\nSet the output queue in the \nconfig.properties\n by setting the \nsqs_queue_uri\n property to full SQS queue uri from AWS console.\n\n\nThe producer uses the \nAWS SQS SDK\n.\n\n\nGoogle Cloud Pub/Sub\n\n\n\n\nIn order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the \nroles/pubsub.publisher\n role.\n\n\nSee the Google Cloud Platform docs for the \nlatest examples of which permissions are needed\n, as well as \nhow to properly configure service accounts\n.\n\n\nSet the output stream in \nconfig.properties\n by setting the \npubsub_project_id\n and \npubsub_topic\n properties. Optionally configure a dedicated output topic\nfor DDL updates by setting the \nddl_pubsub_topic\n property.\n\n\nThe producer uses the \nGoogle Cloud Java Library for Pub/Sub\n and uses its built-in configurations.\n\n\nRabbitMQ\n\n\n\n\nTo produce messages to RabbitMQ, you will need to specify a host in \nconfig.properties\n with \nrabbitmq_host\n. This is the only required property, everything else falls back to a sane default.\n\n\nThe remaining configurable properties are:\n- \nrabbitmq_user\n - defaults to \nguest\n\n- \nrabbitmq_pass\n - defaults to \nguest\n\n- \nrabbitmq_virtual_host\n - defaults to \n/\n\n- \nrabbitmq_exchange\n - defaults to \nmaxwell\n\n- \nrabbitmq_exchange_type\n - defaults to \nfanout\n\n- \nrabbitmq_exchange_durable\n - defaults to \nfalse\n\n- \nrabbitmq_exchange_autodelete\n - defaults to \nfalse\n\n- \nrabbitmq_routing_key_template\n - defaults to \n%db%.%table%\n\n    - This config controls the routing key, where \n%db%\n and \n%table%\n are placeholders that will be substituted at runtime\n- \nrabbitmq_message_persistent\n - defaults to \nfalse\n\n- \nrabbitmq_declare_exchange\n - defaults to \ntrue\n\n\nFor more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html\n\n\nRedis\n\n\n\n\nSet the output stream in \nconfig.properties\n by setting the \nredis_pub_channel\n property for redis_type = pubsub or set the \nredis_list_key\n property when using redis_type = lpush.\n\n\nOther configurable properties are:\n\n\n\n\nredis_host\n - defaults to \nlocalhost\n\n\nredis_port\n - defaults to \n6379\n\n\nredis_auth\n - defaults to \nnull\n\n\nredis_database\n - defaults to \n0\n\n\nredis_type\n - defaults to \npubsub\n\n\nredis_list_key\n - defaults to \nmaxwell\n\n\n\n\nCustom Producer\n\n\n\n\nIf none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. The producer is responsible for processing the raw database rows. Note that your producer may receive DDL and heartbeat rows as well, but your producer can easily filter them out (see example).\n\n\nIn order to register your custom producer, you must implement the \nProducerFactory\n interface, which is responsible for creating your custom \nAbstractProducer\n. Next, set the \ncustom_producer.factory\n configuration property to your \nProducerFactory\n's fully qualified class name. Then add the custom \nProducerFactory\n and all its dependencies to the $MAXWELL_HOME/lib directory.\n\n\nYour custom producer will likely require configuration properties as well. For that, use the \ncustom_producer.*\n property namespace. Those properties will be exposed to your producer via \nMaxwellConfig.customProducerProperties\n.\n\n\nCustom producer factory and producer examples can be found here: \nhttps://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory", 
            "title": "Producers"
        }, 
        {
            "location": "/producers/#kafka", 
            "text": "", 
            "title": "Kafka"
        }, 
        {
            "location": "/producers/#topic", 
            "text": "Maxwell writes to a kafka topic named \"maxwell\" by default. It can be static,\ne.g. 'maxwell', or dynamic, e.g.  namespace_%{database}_%{table} . In the\nlatter case 'database' and 'table' will be replaced with the values for the row\nbeing processed. This can be changed with the  kafka_topic  option.", 
            "title": "Topic"
        }, 
        {
            "location": "/producers/#client-version", 
            "text": "By default, maxwell uses the kafka 1.0.0 library. The  --kafka_version  flag\nlets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or\n0.11.0.1, 1.0.0.  This flag is only available on the command line.   The 0.8.2.2 client is only compatible with brokers running kafka 0.8.  The 0.10.0.x client is only compatible with brokers 0.10.0.x or later.  Mixing the 0.10 client with other versions can lead to serious performance impacts.\n  For More details,  read about it here .  The 0.11.0 client can talk to version 0.10.0 or newer brokers.  The 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case:   ERROR Sender - Uncaught error in kafka producer I/O thread:\nSchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException", 
            "title": "Client version"
        }, 
        {
            "location": "/producers/#extended-options", 
            "text": "Any options given to Maxwell that are prefixed with  kafka.  will be passed directly into the Kafka producer configuration\n(with  kafka.  stripped off).  We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs", 
            "title": "Extended options"
        }, 
        {
            "location": "/producers/#performant-properties", 
            "text": "These properties would give high throughput performance.  kafka.acks = 1\nkafka.compression.type = snappy\nkafka.retries=0", 
            "title": "Performant properties"
        }, 
        {
            "location": "/producers/#at-least-once-properties", 
            "text": "For at-least-once delivery, you will want something more like:  kafka.acks = all\nkafka.retries = 5 # or some larger number  And you will also want to set  min.insync.replicas  on Maxwell's output topic.", 
            "title": "At-least-once properties"
        }, 
        {
            "location": "/producers/#keys", 
            "text": "Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:  {  database : test_tb , table : test_tbl , pk.id :4, pk.part2 : hello }  This key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.  The JSON-hash based key format is tricky to regenerate in a stable fashion.  If you have an\napplication in which you need to parse and re-generate keys, it's advised you enable --kafka_key_format=array , which will generate kafka keys that can be parsed and re-output byte-for-byte.", 
            "title": "Keys"
        }, 
        {
            "location": "/producers/#partitioning", 
            "text": "Both Kafka and AWS Kinesis support the notion of partitioned streams.\nPartitioning is controlled by  producer_partition_by , which gives you the\noption to split your stream by database, table, primary\nkey, or column data.  How you choose to partition will influence the consumers\nof the maxwell stream.  A good rule of thumb is to use the finest-grained\npartition scheme possible given serialization constraints.  To partition by column data, you must set both:   producer_partition_columns , a comma-separated list of column names, and  producer_partiton_by_fallback . This may be ( database ,  table ,\n   primary_key ), and will be used as a default value when the column does not\n  exist.   When partitioning by column Maxwell will treat the values for the specified\ncolumns as strings, concatenate them and use that value to partition the data.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/producers/#kafka-partitioning", 
            "text": "A binlog event's partition is determined by the selected hash function and hash string as follows    HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS  The HASH_FUNCTION is either java's  hashCode  or  murmurhash3 . The default\nHASH_FUNCTION is  hashCode . Murmurhash3 may be set with the kafka_partition_hash  option. The seed value for the murmurhash function is\nhardcoded to 25342 in the MaxwellKafkaPartitioner class.  The HASH_STRING may be ( database ,  table ,  primary_key ,  transaction_id ,  column ).  The\ndefault HASH_STRING is the  database . The partitioning field can be configured\nusing the  producer_partition_by  option.  Maxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:  bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2  http://kafka.apache.org/documentation.html#quickstart", 
            "title": "Kafka partitioning"
        }, 
        {
            "location": "/producers/#kinesis", 
            "text": "", 
            "title": "Kinesis"
        }, 
        {
            "location": "/producers/#aws-credentials", 
            "text": "You will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to:   \"kinesis:PutRecord\"  \"kinesis:PutRecords\"  \"kinesis:DescribeStream\"  \"cloudwatch:PutMetricData\"   See the  AWS docs  for the latest examples on which permissions are needed.  The producer uses the  DefaultAWSCredentialsProviderChain  class to gain aws credentials.\nSee the  AWS docs  on how to setup the IAM user with the Default Credential Provider Chain.", 
            "title": "AWS Credentials"
        }, 
        {
            "location": "/producers/#options", 
            "text": "Set the output stream in  config.properties  by setting the  kinesis_stream  property.  The producer uses the  KPL (Kinesis Producer Library)  and uses the KPL built in configurations.\nCopy  kinesis-producer-library.properties.example  to  kinesis-producer-library.properties  and configure the properties file to your needs.  You are  required  to configure the region. For example:  # set explicitly\nRegion=us-west-2\n# or set with an environment variable\nRegion=$AWS_DEFAULT_REGION  By default, the KPL implements  record aggregation , which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the  KCL (Kinesis Client Library)  to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the  AWS Kinesis Aggregation library ), or disable record aggregation in your  kinesis-producer-library.properties  configuration.  To disable aggregation, add the following to your configuration:  AggregationEnabled=false  Remember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput.", 
            "title": "Options"
        }, 
        {
            "location": "/producers/#sqs", 
            "text": "", 
            "title": "SQS"
        }, 
        {
            "location": "/producers/#aws-credentials_1", 
            "text": "You will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses  DefaultAWSCredentialsProviderChain  to get AWS credentials.  See the  AWS docs  on how to setup the IAM user with the Default Credential Provider Chain.  In case you need to set up a different region also along with credentials then default one, see the  AWS docs .", 
            "title": "AWS Credentials"
        }, 
        {
            "location": "/producers/#options_1", 
            "text": "Set the output queue in the  config.properties  by setting the  sqs_queue_uri  property to full SQS queue uri from AWS console.  The producer uses the  AWS SQS SDK .", 
            "title": "Options"
        }, 
        {
            "location": "/producers/#google-cloud-pubsub", 
            "text": "In order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the  roles/pubsub.publisher  role.  See the Google Cloud Platform docs for the  latest examples of which permissions are needed , as well as  how to properly configure service accounts .  Set the output stream in  config.properties  by setting the  pubsub_project_id  and  pubsub_topic  properties. Optionally configure a dedicated output topic\nfor DDL updates by setting the  ddl_pubsub_topic  property.  The producer uses the  Google Cloud Java Library for Pub/Sub  and uses its built-in configurations.", 
            "title": "Google Cloud Pub/Sub"
        }, 
        {
            "location": "/producers/#rabbitmq", 
            "text": "To produce messages to RabbitMQ, you will need to specify a host in  config.properties  with  rabbitmq_host . This is the only required property, everything else falls back to a sane default.  The remaining configurable properties are:\n-  rabbitmq_user  - defaults to  guest \n-  rabbitmq_pass  - defaults to  guest \n-  rabbitmq_virtual_host  - defaults to  / \n-  rabbitmq_exchange  - defaults to  maxwell \n-  rabbitmq_exchange_type  - defaults to  fanout \n-  rabbitmq_exchange_durable  - defaults to  false \n-  rabbitmq_exchange_autodelete  - defaults to  false \n-  rabbitmq_routing_key_template  - defaults to  %db%.%table% \n    - This config controls the routing key, where  %db%  and  %table%  are placeholders that will be substituted at runtime\n-  rabbitmq_message_persistent  - defaults to  false \n-  rabbitmq_declare_exchange  - defaults to  true  For more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html", 
            "title": "RabbitMQ"
        }, 
        {
            "location": "/producers/#redis", 
            "text": "Set the output stream in  config.properties  by setting the  redis_pub_channel  property for redis_type = pubsub or set the  redis_list_key  property when using redis_type = lpush.  Other configurable properties are:   redis_host  - defaults to  localhost  redis_port  - defaults to  6379  redis_auth  - defaults to  null  redis_database  - defaults to  0  redis_type  - defaults to  pubsub  redis_list_key  - defaults to  maxwell", 
            "title": "Redis"
        }, 
        {
            "location": "/producers/#custom-producer", 
            "text": "If none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. The producer is responsible for processing the raw database rows. Note that your producer may receive DDL and heartbeat rows as well, but your producer can easily filter them out (see example).  In order to register your custom producer, you must implement the  ProducerFactory  interface, which is responsible for creating your custom  AbstractProducer . Next, set the  custom_producer.factory  configuration property to your  ProducerFactory 's fully qualified class name. Then add the custom  ProducerFactory  and all its dependencies to the $MAXWELL_HOME/lib directory.  Your custom producer will likely require configuration properties as well. For that, use the  custom_producer.*  property namespace. Those properties will be exposed to your producer via  MaxwellConfig.customProducerProperties .  Custom producer factory and producer examples can be found here:  https://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory", 
            "title": "Custom Producer"
        }, 
        {
            "location": "/filtering/", 
            "text": "Basic Filters\n\n\n\n\nMaxwell can be configured to filter out updates from specific tables.  This is controlled\nby the \n--filter\n command line flag.\n\n\nExample 1\n\n\n--filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/'\n\n\n\n\nThis example tells Maxwell to suppress all updates that happen on \nfoodb\n, except for updates\nto \ntbl\n and any table in foodb matching the regexp \n/table_\\d+/\n.\n\n\nExample 2\n\n\nFilter options are evaluated in the order specified, so in this example we\nsuppress everything except updates in the \ndb1\n database.\n\n\n--filter = 'exclude: *.*, include: db1.*'\n\n\n\n\nColumn Filters\n\n\n\n\nMaxwell can also include/exclude based on column values:\n\n\n--filter = 'exclude: db.tbl.col = reject'\n\n\n\n\nwill reject any row in \ndb.tbl\n that contains \ncol\n and where the stringified value of \"col\" is \"reject\".\nColumn filters are ignored if the specified column is not present, so \n--filter = 'exclude: *.*.col_a = *'\n\nwill exclude updates to any table that contains \ncol_a\n, but include every other table.\n\n\nBlacklisting\n\n\n\n\nIn rare cases, you may wish to tell Maxwell to completely ignore a database or\ntable, including schema changes.  In general, don't use this.  If you must use this:\n\n\n--filter = 'blacklist: bad_db.*'\n\n\n\n\nNote that once Maxwell has been running with a table or database marked as\nblacklisted, you \nmust\n continue to run Maxwell with that table or database\nblacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.\nAlso note that this is the feature I most regret writing.\n\n\nJavascript Filters\n\n\n\n\nIf you need more flexibility than the native filters provide, you can write a small chunk of\njavascript for Maxwell to pass each row through with \n--javascript FILE\n.  This file should contain\nat least a javascript function named \nprocess_row\n.  This function will be passed a \nWrappedRowMap\n\nobject and is free to make filtering and data munging decisions:\n\n\nfunction process_row(row) {\n    if ( row.database == \ntest\n \n row.table == \nbar\n ) {\n        var username = row.data.get(\nusername\n);\n        if ( username == \nosheroff\n )\n            row.suppress();\n\n        row.data.put(\nusername\n, username.toUpperCase());\n    }\n}\n\n\n\n\nThere's a longer example here: \nhttps://github.com/zendesk/maxwell/blob/master/src/example/filter.js\n.", 
            "title": "Filtering"
        }, 
        {
            "location": "/filtering/#basic-filters", 
            "text": "Maxwell can be configured to filter out updates from specific tables.  This is controlled\nby the  --filter  command line flag.", 
            "title": "Basic Filters"
        }, 
        {
            "location": "/filtering/#example-1", 
            "text": "--filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/'  This example tells Maxwell to suppress all updates that happen on  foodb , except for updates\nto  tbl  and any table in foodb matching the regexp  /table_\\d+/ .", 
            "title": "Example 1"
        }, 
        {
            "location": "/filtering/#example-2", 
            "text": "Filter options are evaluated in the order specified, so in this example we\nsuppress everything except updates in the  db1  database.  --filter = 'exclude: *.*, include: db1.*'", 
            "title": "Example 2"
        }, 
        {
            "location": "/filtering/#column-filters", 
            "text": "Maxwell can also include/exclude based on column values:  --filter = 'exclude: db.tbl.col = reject'  will reject any row in  db.tbl  that contains  col  and where the stringified value of \"col\" is \"reject\".\nColumn filters are ignored if the specified column is not present, so  --filter = 'exclude: *.*.col_a = *' \nwill exclude updates to any table that contains  col_a , but include every other table.", 
            "title": "Column Filters"
        }, 
        {
            "location": "/filtering/#blacklisting", 
            "text": "In rare cases, you may wish to tell Maxwell to completely ignore a database or\ntable, including schema changes.  In general, don't use this.  If you must use this:  --filter = 'blacklist: bad_db.*'  Note that once Maxwell has been running with a table or database marked as\nblacklisted, you  must  continue to run Maxwell with that table or database\nblacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.\nAlso note that this is the feature I most regret writing.", 
            "title": "Blacklisting"
        }, 
        {
            "location": "/filtering/#javascript-filters", 
            "text": "If you need more flexibility than the native filters provide, you can write a small chunk of\njavascript for Maxwell to pass each row through with  --javascript FILE .  This file should contain\nat least a javascript function named  process_row .  This function will be passed a  WrappedRowMap \nobject and is free to make filtering and data munging decisions:  function process_row(row) {\n    if ( row.database ==  test    row.table ==  bar  ) {\n        var username = row.data.get( username );\n        if ( username ==  osheroff  )\n            row.suppress();\n\n        row.data.put( username , username.toUpperCase());\n    }\n}  There's a longer example here:  https://github.com/zendesk/maxwell/blob/master/src/example/filter.js .", 
            "title": "Javascript Filters"
        }, 
        {
            "location": "/dataformat/", 
            "text": "So you ran some sql?\n\n\ncreate table test.e (\n  id int(10) not null primary key auto_increment,\n  m double,\n  c timestamp(6),\n  comment varchar(255) charset 'latin1'\n);\n\ninsert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\nupdate test.e set m = 5.444, c = now(3) where id = 1;\ndelete from test.e where id = 1;\nalter table test.e add column torvalds bigint unsigned after m;\ndrop table test.e;\n\n\n\nMaxwell will produce some output for that.  Let's look at it.\n\n\nINSERT\n\n\n\n\nmysql\n insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\ninsert\n,\n   \nts\n:1477053217,\n   \nxid\n:23396,\n   \ncommit\n:true,\n   \nposition\n:\nmaster.000006:800911\n,\n   \nserver_id\n:23042,\n   \nthread_id\n:108,\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:4.2341,\n      \nc\n:\n2016-10-21 05:33:37.523000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   }\n}\n\n\n\n\n\nMost of the fields are self-explanatory, but a couple of them deserve mention:\n\n\n\u21b3 \n\"type\":\"insert\",\n\n\nMost commonly you will see insert/update/delete here.  If you're bootstrapping\na table, you will see \"bootstrap-insert\", and DDL statements (explained later)\nhave their own types.\n\n\n\u21b3 \n\"xid\":23396,\n\n\nThis is InnoDB's \"transaction ID\" for the transaction this row is associated\nwith.  It's unique within the lifetime of a server as near as I can tell.\n\n\n\u21b3 \n\"server_id\":23042,\n\n\nThe mysql server_id of the server that accepted this transaction.\n\n\n\u21b3 \n\"thread_id\":108,\n\n\nA thread_id is more or less a unique identifier of the client connection that\ngenerated the data.\n\n\n\u21b3 \n\"commit\":true,\n\n\nIf you need to re-assemble transactions in your stream processors, you can use\nthis field and \nxid\n to do so.  The data will look like:\n\n\n\n\nrow with no \ncommit\n, xid=142\n\n\nrow with no \ncommit\n, xid=142\n\n\nrow with \ncommit=true\n, xid=142\n\n\nrow with no \ncommit\n, xid=155\n\n\n...\n\n\n\n\nUPDATE\n\n\n\n\nmysql\n update test.e set m = 5.444, c = now(3) where id = 1;\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\nupdate\n,\n   \nts\n:1477053234,\n   ...\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:5.444,\n      \nc\n:\n2016-10-21 05:33:54.631000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   },\n   \nold\n:{\n      \nm\n:4.2341,\n      \nc\n:\n2016-10-21 05:33:37.523000\n\n   }\n}\n\n\n\n\n\nWhat's important to note here is the \nold\n field, which stores old values for\nrows that changed.  So \ndata\n still has a complete copy of the row (just as\nwith the insert), but now you can reconstruct what the row \nwas\n by doing\n\ndata.merge(old)\n.\n\n\nDELETE\n\n\n\n\nmysql\n delete from test.e where id = 1;\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\ndelete\n,\n   ...\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:5.444,\n      \nc\n:\n2016-10-21 05:33:54.631000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   }\n}\n\n\n\n\nafter a DELETE, \ndata\n contains a copy of the row, just before it shuffled off\nthis mortal coil.\n\n\nCREATE TABLE\n\n\n\n\ncreate table test.e ( ... )\n{\n   \ntype\n:\ntable-create\n,\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ndef\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \nts\n:1477053126000,\n   \nsql\n:\ncreate table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\n,\n   \nposition\n:\nmaster.000006:800050\n\n}\n\n\n\n\n\nYou only get this with \n--output_ddl\n.\n\n\n\u21b3 \n\"type\": \"table-create\"\n\nhere you have \ndatabase-create\n, \ndatabase-alter\n, \ndatabase-drop\n, \ntable-create\n, \ntable-alter\n, \ntable-drop\n.\n\n\n\u21b3 \n\"type\":\"int\",\n\nMostly here we preserve the inbound type of the column.  There's a couple of\nexceptions where we will change the column type, you could read about them in the\n\nunalias_type\n\nfunction if you so desired.\n\n\nALTER TABLE\n\n\nmysql\n alter table test.e add column torvalds bigint unsigned after m;\n{\n   \ntype\n:\ntable-alter\n,\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \nold\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \ndef\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\nbigint\n,\n            \nname\n:\ntorvalds\n,\n            \nsigned\n:false\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \nts\n:1477053308000,\n   \nsql\n:\nalter table test.e add column torvalds bigint unsigned after m\n,\n   \nposition\n:\nmaster.000006:804398\n\n}\n\n\n\n\nAs with the CREATE TABLE, we have a complete image of the table before-and-after the alter\n\n\nblob (+ binary encoded strings)\n\n\n\n\nMaxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).\n\n\ndatetime\n\n\n\n\nDatetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.\n\n\nmysql\n    create table test_datetime ( id int(11), dtcol datetime );\nmysql\n    insert into test_datetime set dtcol='0000-00-00 00:00:00';\n\n\nmaxwell  { \ntable\n : \ntest_datetime\n, \ntype\n: \ninsert\n, \ndata\n: { \ndtcol\n: \n0000-00-00 00:00:00\n } }\n\n\n\n\nAs of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.\n\n\nsets\n\n\n\n\noutput as JSON arrays.\n\n\nmysql\n   create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql\n   insert into test_sets set setcol = 'b_val,c_val';\n\n\nmaxwell { \ntable\n:\ntest_sets\n, \ntype\n:\ninsert\n, \ndata\n: { \nsetcol\n: [\nb_val\n, \nc_val\n] } }\n\n\n\n\nstrings (varchar, text)\n\n\n\n\nMaxwell will accept a variety of character encodings, but will always output UTF-8 strings.  The following table\ndescribes support for mysql's character sets:\n\n\n\n\n\n\n\n\ncharset\n\n\nstatus\n\n\n\n\n\n\n\n\n\n\nutf8\n\n\nsupported\n\n\n\n\n\n\nutf8mb4\n\n\nsupported\n\n\n\n\n\n\nlatin1\n\n\nsupported\n\n\n\n\n\n\nlatin2\n\n\nsupported\n\n\n\n\n\n\nascii\n\n\nsupported\n\n\n\n\n\n\nucs2\n\n\nsupported\n\n\n\n\n\n\nbinary\n\n\nsupported (as base64)\n\n\n\n\n\n\nutf16\n\n\nsupported, not tested in production\n\n\n\n\n\n\nutf32\n\n\nsupported, not tested in production\n\n\n\n\n\n\nbig5\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp850\n\n\nsupported, not tested in production\n\n\n\n\n\n\nsjis\n\n\nsupported, not tested in production\n\n\n\n\n\n\nhebrew\n\n\nsupported, not tested in production\n\n\n\n\n\n\ntis620\n\n\nsupported, not tested in production\n\n\n\n\n\n\neuckr\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngb2312\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngreek\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1250\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngbk\n\n\nsupported, not tested in production\n\n\n\n\n\n\nlatin5\n\n\nsupported, not tested in production\n\n\n\n\n\n\nmacroman\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp852\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1251\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp866\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1256\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1257\n\n\nsupported, not tested in production\n\n\n\n\n\n\ndec8\n\n\nunsupported\n\n\n\n\n\n\nhp8\n\n\nunsupported\n\n\n\n\n\n\nkoi8r\n\n\nunsupported\n\n\n\n\n\n\nswe7\n\n\nunsupported\n\n\n\n\n\n\nujis\n\n\nunsupported\n\n\n\n\n\n\nkoi8u\n\n\nunsupported\n\n\n\n\n\n\narmscii8\n\n\nunsupported\n\n\n\n\n\n\nkeybcs2\n\n\nunsupported\n\n\n\n\n\n\nmacce\n\n\nunsupported\n\n\n\n\n\n\nlatin7\n\n\nunsupported\n\n\n\n\n\n\ngeostd8\n\n\nunsupported\n\n\n\n\n\n\ncp932\n\n\nunsupported\n\n\n\n\n\n\neucjpms\n\n\nunsupported", 
            "title": "Data Format"
        }, 
        {
            "location": "/dataformat/#so-you-ran-some-sql", 
            "text": "create table test.e (\n  id int(10) not null primary key auto_increment,\n  m double,\n  c timestamp(6),\n  comment varchar(255) charset 'latin1'\n);\n\ninsert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\nupdate test.e set m = 5.444, c = now(3) where id = 1;\ndelete from test.e where id = 1;\nalter table test.e add column torvalds bigint unsigned after m;\ndrop table test.e;  Maxwell will produce some output for that.  Let's look at it.", 
            "title": "So you ran some sql?"
        }, 
        {
            "location": "/dataformat/#insert", 
            "text": "mysql  insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\n{\n    database : test ,\n    table : e ,\n    type : insert ,\n    ts :1477053217,\n    xid :23396,\n    commit :true,\n    position : master.000006:800911 ,\n    server_id :23042,\n    thread_id :108,\n    data :{\n       id :1,\n       m :4.2341,\n       c : 2016-10-21 05:33:37.523000 ,\n       comment : I am a creature of light. \n   }\n}  Most of the fields are self-explanatory, but a couple of them deserve mention:  \u21b3  \"type\":\"insert\",  Most commonly you will see insert/update/delete here.  If you're bootstrapping\na table, you will see \"bootstrap-insert\", and DDL statements (explained later)\nhave their own types.  \u21b3  \"xid\":23396,  This is InnoDB's \"transaction ID\" for the transaction this row is associated\nwith.  It's unique within the lifetime of a server as near as I can tell.  \u21b3  \"server_id\":23042,  The mysql server_id of the server that accepted this transaction.  \u21b3  \"thread_id\":108,  A thread_id is more or less a unique identifier of the client connection that\ngenerated the data.  \u21b3  \"commit\":true,  If you need to re-assemble transactions in your stream processors, you can use\nthis field and  xid  to do so.  The data will look like:   row with no  commit , xid=142  row with no  commit , xid=142  row with  commit=true , xid=142  row with no  commit , xid=155  ...", 
            "title": "INSERT"
        }, 
        {
            "location": "/dataformat/#update", 
            "text": "mysql  update test.e set m = 5.444, c = now(3) where id = 1;\n{\n    database : test ,\n    table : e ,\n    type : update ,\n    ts :1477053234,\n   ...\n    data :{\n       id :1,\n       m :5.444,\n       c : 2016-10-21 05:33:54.631000 ,\n       comment : I am a creature of light. \n   },\n    old :{\n       m :4.2341,\n       c : 2016-10-21 05:33:37.523000 \n   }\n}  What's important to note here is the  old  field, which stores old values for\nrows that changed.  So  data  still has a complete copy of the row (just as\nwith the insert), but now you can reconstruct what the row  was  by doing data.merge(old) .", 
            "title": "UPDATE"
        }, 
        {
            "location": "/dataformat/#delete", 
            "text": "mysql  delete from test.e where id = 1;\n{\n    database : test ,\n    table : e ,\n    type : delete ,\n   ...\n    data :{\n       id :1,\n       m :5.444,\n       c : 2016-10-21 05:33:54.631000 ,\n       comment : I am a creature of light. \n   }\n}  after a DELETE,  data  contains a copy of the row, just before it shuffled off\nthis mortal coil.", 
            "title": "DELETE"
        }, 
        {
            "location": "/dataformat/#create-table", 
            "text": "create table test.e ( ... )\n{\n    type : table-create ,\n    database : test ,\n    table : e ,\n    def :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    ts :1477053126000,\n    sql : create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ) ,\n    position : master.000006:800050 \n}  You only get this with  --output_ddl .  \u21b3  \"type\": \"table-create\" \nhere you have  database-create ,  database-alter ,  database-drop ,  table-create ,  table-alter ,  table-drop .  \u21b3  \"type\":\"int\", \nMostly here we preserve the inbound type of the column.  There's a couple of\nexceptions where we will change the column type, you could read about them in the unalias_type \nfunction if you so desired.", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/dataformat/#alter-table", 
            "text": "mysql  alter table test.e add column torvalds bigint unsigned after m;\n{\n    type : table-alter ,\n    database : test ,\n    table : e ,\n    old :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    def :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : bigint ,\n             name : torvalds ,\n             signed :false\n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    ts :1477053308000,\n    sql : alter table test.e add column torvalds bigint unsigned after m ,\n    position : master.000006:804398 \n}  As with the CREATE TABLE, we have a complete image of the table before-and-after the alter", 
            "title": "ALTER TABLE"
        }, 
        {
            "location": "/dataformat/#blob-binary-encoded-strings", 
            "text": "Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).", 
            "title": "blob (+ binary encoded strings)"
        }, 
        {
            "location": "/dataformat/#datetime", 
            "text": "Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.  mysql     create table test_datetime ( id int(11), dtcol datetime );\nmysql     insert into test_datetime set dtcol='0000-00-00 00:00:00'; maxwell  {  table  :  test_datetime ,  type :  insert ,  data : {  dtcol :  0000-00-00 00:00:00  } }  As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.", 
            "title": "datetime"
        }, 
        {
            "location": "/dataformat/#sets", 
            "text": "output as JSON arrays.  mysql    create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql    insert into test_sets set setcol = 'b_val,c_val'; maxwell {  table : test_sets ,  type : insert ,  data : {  setcol : [ b_val ,  c_val ] } }", 
            "title": "sets"
        }, 
        {
            "location": "/dataformat/#strings-varchar-text", 
            "text": "Maxwell will accept a variety of character encodings, but will always output UTF-8 strings.  The following table\ndescribes support for mysql's character sets:     charset  status      utf8  supported    utf8mb4  supported    latin1  supported    latin2  supported    ascii  supported    ucs2  supported    binary  supported (as base64)    utf16  supported, not tested in production    utf32  supported, not tested in production    big5  supported, not tested in production    cp850  supported, not tested in production    sjis  supported, not tested in production    hebrew  supported, not tested in production    tis620  supported, not tested in production    euckr  supported, not tested in production    gb2312  supported, not tested in production    greek  supported, not tested in production    cp1250  supported, not tested in production    gbk  supported, not tested in production    latin5  supported, not tested in production    macroman  supported, not tested in production    cp852  supported, not tested in production    cp1251  supported, not tested in production    cp866  supported, not tested in production    cp1256  supported, not tested in production    cp1257  supported, not tested in production    dec8  unsupported    hp8  unsupported    koi8r  unsupported    swe7  unsupported    ujis  unsupported    koi8u  unsupported    armscii8  unsupported    keybcs2  unsupported    macce  unsupported    latin7  unsupported    geostd8  unsupported    cp932  unsupported    eucjpms  unsupported", 
            "title": "strings (varchar, text)"
        }, 
        {
            "location": "/encryption/", 
            "text": "Using encryption\n\n\n\n\nWhen encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key.\nValues are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message\n\n\nDecryption\n\n\n\n\nTo decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt().\n\n\nExamples\n\n\n\n\ninsert into minimal set account_id =1, text_field='hello'\n\n\nencrypt=none\n (unencrypted):\n\n\n{\ndatabase\n:\nshard_1\n,\ntable\n:\nminimal\n,\ntype\n:\ninsert\n,\nts\n:1490115785,\nxid\n:153,\ncommit\n:true,\ndata\n:{\nid\n:1,\naccount_id\n:1,\ntext_field\n:\nhello\n}}\n\n\n\n\nencrypt=data\n:\n\n\n{\ndatabase\n:\nshard_1\n,\ntable\n:\nminimal\n,\ntype\n:\ninsert\n,\nts\n:1504585129,\nxid\n:161,\ncommit\n:true,\nencrypted\n:{\niv\n:\nlqiXoTdz6jed3XgJPpa7EQ==\n,\nbytes\n:\n1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA==\n}}\n\n\n\n\nencrypt=all\n:\n\n\n{\nencrypted\n:{\nbytes\n:\niZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA==\n,\niv\n:\nXXs6AePsXJWAAIrKyLlR0g==\n}}", 
            "title": "Encryption"
        }, 
        {
            "location": "/encryption/#using-encryption", 
            "text": "When encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key.\nValues are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message", 
            "title": "Using encryption"
        }, 
        {
            "location": "/encryption/#decryption", 
            "text": "To decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt().", 
            "title": "Decryption"
        }, 
        {
            "location": "/encryption/#examples", 
            "text": "insert into minimal set account_id =1, text_field='hello'  encrypt=none  (unencrypted):  { database : shard_1 , table : minimal , type : insert , ts :1490115785, xid :153, commit :true, data :{ id :1, account_id :1, text_field : hello }}  encrypt=data :  { database : shard_1 , table : minimal , type : insert , ts :1504585129, xid :161, commit :true, encrypted :{ iv : lqiXoTdz6jed3XgJPpa7EQ== , bytes : 1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA== }}  encrypt=all :  { encrypted :{ bytes : iZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA== , iv : XXs6AePsXJWAAIrKyLlR0g== }}", 
            "title": "Examples"
        }, 
        {
            "location": "/bootstrapping/", 
            "text": "Using the maxwell-bootstrap utility\n\n\n\n\nYou can use the \nmaxwell-bootstrap\n utility to bootstrap tables from the command-line.\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n--log_level LOG_LEVEL\n\n\nlog level (DEBUG, INFO, WARN or ERROR)\n\n\n\n\n\n\n--user USER\n\n\nmysql username\n\n\n\n\n\n\n--password PASSWORD\n\n\nmysql password\n\n\n\n\n\n\n--host HOST\n\n\nmysql host\n\n\n\n\n\n\n--port PORT\n\n\nmysql port\n\n\n\n\n\n\n--database DATABASE\n\n\nmysql database containing the table to bootstrap\n\n\n\n\n\n\n--table TABLE\n\n\nmysql table to bootstrap\n\n\n\n\n\n\n--where WHERE_CLAUSE\n\n\nwhere clause to restrict the rows bootstrapped from the specified table\n\n\n\n\n\n\n--client_id CLIENT_ID\n\n\nspecify which maxwell instance should perform the bootstrap operation\n\n\n\n\n\n\n\n\nUsing the maxwell.bootstrap table\n\n\n\n\nAlternatively you can insert a row in the \nmaxwell.bootstrap\n table to trigger a bootstrap.\n\n\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');\n\n\n\n\nOptionally, you can include a where clause to replay part of the data.\n\n\nbin/maxwell-bootstrap --config localhost.properties --database foobar --table test --log_level info\n\n\nor\n\n\nbin/maxwell-bootstrap --config localhost.properties --database foobar --table test --where \"my_date \n= '2017-01-07 00:00:00'\" --log_level info\n\n\nAsync vs Sync bootstrapping\n\n\n\n\nThe Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with \n--bootstrapper=sync\n, the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with \n--bootstrapper=async\n however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.\n\n\nBootstrapping Data Format\n\n\n\n\n\n\na bootstrap starts with an event of \ntype = \"bootstrap-start\"\n\n\nthen events with \ntype = \"bootstrap-insert\"\n (one per row in the table)\n\n\nthen one event per \nINSERT\n, \nUPDATE\n or \nDELETE\n with standard event types i.e. \ntype = \"insert\"\n, \ntype = \"update\"\n or \ntype = \"delete\"\n that occurred since the beginning of bootstrap\n\n\nfinally an event with \ntype = \"bootstrap-complete\"\n\n\n\n\nHere's a complete example:\n\n\nmysql\n create table fooDB.barTable(txt varchar(255));\nmysql\n insert into fooDB.barTable (txt) values (\nhello\n), (\nbootstrap!\n);\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values (\nfooDB\n, \nbarTable\n);\n\n\n\n\nCorresponding replication stream output of table \nfooDB.barTable\n:\n\n\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-start\n,\nts\n:1450557744,\ndata\n:{}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-complete\n,\nts\n:1450557744,\ndata\n:{}}\n\n\n\n\nFailure Scenarios\n\n\n\n\nIf Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress.\nIf this behavior is not desired, manual updates to the \nbootstrap\n table are required.\nSpecifically, marking the unfinished bootstrap row as 'complete' (\nis_complete\n = 1) or deleting the row.\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwell-bootstrap-utility", 
            "text": "You can use the  maxwell-bootstrap  utility to bootstrap tables from the command-line.     option  description      --log_level LOG_LEVEL  log level (DEBUG, INFO, WARN or ERROR)    --user USER  mysql username    --password PASSWORD  mysql password    --host HOST  mysql host    --port PORT  mysql port    --database DATABASE  mysql database containing the table to bootstrap    --table TABLE  mysql table to bootstrap    --where WHERE_CLAUSE  where clause to restrict the rows bootstrapped from the specified table    --client_id CLIENT_ID  specify which maxwell instance should perform the bootstrap operation", 
            "title": "Using the maxwell-bootstrap utility"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwellbootstrap-table", 
            "text": "Alternatively you can insert a row in the  maxwell.bootstrap  table to trigger a bootstrap.  mysql  insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');  Optionally, you can include a where clause to replay part of the data.  bin/maxwell-bootstrap --config localhost.properties --database foobar --table test --log_level info  or  bin/maxwell-bootstrap --config localhost.properties --database foobar --table test --where \"my_date  = '2017-01-07 00:00:00'\" --log_level info", 
            "title": "Using the maxwell.bootstrap table"
        }, 
        {
            "location": "/bootstrapping/#async-vs-sync-bootstrapping", 
            "text": "The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with  --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with  --bootstrapper=async  however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.", 
            "title": "Async vs Sync bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#bootstrapping-data-format", 
            "text": "a bootstrap starts with an event of  type = \"bootstrap-start\"  then events with  type = \"bootstrap-insert\"  (one per row in the table)  then one event per  INSERT ,  UPDATE  or  DELETE  with standard event types i.e.  type = \"insert\" ,  type = \"update\"  or  type = \"delete\"  that occurred since the beginning of bootstrap  finally an event with  type = \"bootstrap-complete\"   Here's a complete example:  mysql  create table fooDB.barTable(txt varchar(255));\nmysql  insert into fooDB.barTable (txt) values ( hello ), ( bootstrap! );\nmysql  insert into maxwell.bootstrap (database_name, table_name) values ( fooDB ,  barTable );  Corresponding replication stream output of table  fooDB.barTable :  { database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-start , ts :1450557744, data :{}}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-complete , ts :1450557744, data :{}}", 
            "title": "Bootstrapping Data Format"
        }, 
        {
            "location": "/bootstrapping/#failure-scenarios", 
            "text": "If Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress.\nIf this behavior is not desired, manual updates to the  bootstrap  table are required.\nSpecifically, marking the unfinished bootstrap row as 'complete' ( is_complete  = 1) or deleting the row.  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Failure Scenarios"
        }, 
        {
            "location": "/monitoring/", 
            "text": "Monitoring\n\n\n\n\nMaxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options\nor the \nconfig.properties\n file. These can provide insight into system health.\nAt present certain metrics are Kafka-specific - that is, not yet supported other producers.\n\n\nMetrics\n\n\n\n\nAll metrics are prepended with the configured \nmetrics_prefix.\n\n\n\n\n\n\n\n\nmetric\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nCounters\n\n\n\n\n\n\n\n\nmessages.succeeded\n\n\ncount of messages that were successfully sent to Kafka\n\n\n\n\n\n\nmessages.failed\n\n\ncount of messages that failed to send to Kafka\n\n\n\n\n\n\nrow.count\n\n\na count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka.\n\n\n\n\n\n\nMeters\n\n\n\n\n\n\n\n\nmessages.succeeded.meter\n\n\na measure of the rate at which messages were successfully sent to Kafka\n\n\n\n\n\n\nmessages.failed.meter\n\n\na measure of the rate at which messages failed to send Kafka\n\n\n\n\n\n\nrow.meter\n\n\na measure of the rate at which rows arrive to Maxwell from the binlog connector\n\n\n\n\n\n\nGauges\n\n\n\n\n\n\n\n\nreplication.lag\n\n\nthe time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds\n\n\n\n\n\n\ninflightmessages.count\n\n\nthe number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are)\n\n\n\n\n\n\nTimers\n\n\n\n\n\n\n\n\nmessage.publish.time\n\n\nthe time it took to send a given record to Kafka, in milliseconds\n\n\n\n\n\n\nmessage.publish.age\n\n\nthe time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms.\n\n\n\n\n\n\nreplication.queue.time\n\n\nthe time it took to enqueue a given binlog event for processing, in milliseconds\n\n\n\n\n\n\n\n\nHTTP Endpoints\n\n\n\n\nWhen the HTTP server is enabled the following endpoints are exposed:\n\n\n\n\n\n\n\n\nendpoint\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n/metrics\n\n\nreturn all metrics as JSON\n\n\n\n\n\n\n/prometheus\n\n\nreturn all metrics as Prometheus format\n\n\n\n\n\n\n/healthcheck\n\n\nrun Maxwell's healthchecks.  Considered unhealthy if \n0 messages have failed in the last 15 minutes.\n\n\n\n\n\n\n/ping\n\n\na simple ping test, responds with \npong\n\n\n\n\n\n\n\n\nJMX Configuration\n\n\n\n\nStandard configuration is either via commandline args or the \nconfig.properties\n file. However, when exposing JMX metrics\nadditional configuration is required to allow remote access. In this case Maxwell makes use of the \nJAVA_OPTS\n environment variable.\nTo make use of this set \nJAVA_OPTS\n before starting Maxwell.\n\n\nThe following is an example which allows remote access with no authentication and insecure connections.\n\n\nexport JAVA_OPTS=\n-Dcom.sun.management.jmxremote \\\n-Dcom.sun.management.jmxremote.port=9010 \\\n-Dcom.sun.management.jmxremote.local.only=false \\\n-Dcom.sun.management.jmxremote.authenticate=false \\\n-Dcom.sun.management.jmxremote.ssl=false \\\n-Djava.rmi.server.hostname=SERVER_IP_ADDRESS", 
            "title": "Monitoring"
        }, 
        {
            "location": "/monitoring/#monitoring", 
            "text": "Maxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options\nor the  config.properties  file. These can provide insight into system health.\nAt present certain metrics are Kafka-specific - that is, not yet supported other producers.", 
            "title": "Monitoring"
        }, 
        {
            "location": "/monitoring/#metrics", 
            "text": "All metrics are prepended with the configured  metrics_prefix.     metric  description      Counters     messages.succeeded  count of messages that were successfully sent to Kafka    messages.failed  count of messages that failed to send to Kafka    row.count  a count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka.    Meters     messages.succeeded.meter  a measure of the rate at which messages were successfully sent to Kafka    messages.failed.meter  a measure of the rate at which messages failed to send Kafka    row.meter  a measure of the rate at which rows arrive to Maxwell from the binlog connector    Gauges     replication.lag  the time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds    inflightmessages.count  the number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are)    Timers     message.publish.time  the time it took to send a given record to Kafka, in milliseconds    message.publish.age  the time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms.    replication.queue.time  the time it took to enqueue a given binlog event for processing, in milliseconds", 
            "title": "Metrics"
        }, 
        {
            "location": "/monitoring/#http-endpoints", 
            "text": "When the HTTP server is enabled the following endpoints are exposed:     endpoint  description      /metrics  return all metrics as JSON    /prometheus  return all metrics as Prometheus format    /healthcheck  run Maxwell's healthchecks.  Considered unhealthy if  0 messages have failed in the last 15 minutes.    /ping  a simple ping test, responds with  pong", 
            "title": "HTTP Endpoints"
        }, 
        {
            "location": "/monitoring/#jmx-configuration", 
            "text": "Standard configuration is either via commandline args or the  config.properties  file. However, when exposing JMX metrics\nadditional configuration is required to allow remote access. In this case Maxwell makes use of the  JAVA_OPTS  environment variable.\nTo make use of this set  JAVA_OPTS  before starting Maxwell.  The following is an example which allows remote access with no authentication and insecure connections.  export JAVA_OPTS= -Dcom.sun.management.jmxremote \\\n-Dcom.sun.management.jmxremote.port=9010 \\\n-Dcom.sun.management.jmxremote.local.only=false \\\n-Dcom.sun.management.jmxremote.authenticate=false \\\n-Dcom.sun.management.jmxremote.ssl=false \\\n-Djava.rmi.server.hostname=SERVER_IP_ADDRESS", 
            "title": "JMX Configuration"
        }, 
        {
            "location": "/embedding/", 
            "text": "Embedding Maxwell\n\n\n\n\nMaxwell typically runs as a command-line program. However, for advanced use it\nis possible to run maxwell from any JVM-based language. Currently the source of\ntruth is the source code (there is no published API documentation). Pull requests\nto better document embedded Maxwell uses are welcome.\n\n\nCompatibility caveat\n\n\n\n\nMaxwell makes every attempt to remain backwards compatible. However this\nonly applies to the command-line usage - Maxwell's Java API may change without\nnotice.\n\n\nHowever (and unless otherwise indicated) breaking API changes will result in\na type error - i.e. if your code still compiles, then the API has not changed.", 
            "title": "Embedding"
        }, 
        {
            "location": "/embedding/#embedding-maxwell", 
            "text": "Maxwell typically runs as a command-line program. However, for advanced use it\nis possible to run maxwell from any JVM-based language. Currently the source of\ntruth is the source code (there is no published API documentation). Pull requests\nto better document embedded Maxwell uses are welcome.", 
            "title": "Embedding Maxwell"
        }, 
        {
            "location": "/embedding/#compatibility-caveat", 
            "text": "Maxwell makes every attempt to remain backwards compatible. However this\nonly applies to the command-line usage - Maxwell's Java API may change without\nnotice.  However (and unless otherwise indicated) breaking API changes will result in\na type error - i.e. if your code still compiles, then the API has not changed.", 
            "title": "Compatibility caveat"
        }, 
        {
            "location": "/schemas/", 
            "text": "Schema storage\n\n\n\n\nThis document describes Maxwell's internal schema storage mechanism. Most users won't need to know how this works, but it can be useful in the rare case where something goes wrong.\n\n\nMySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc.\n\n\nThere is a set of \"base schema\" tables in the maxwell database - \ntables\n, \ncolumns\n, \ndatabases\n. This is where we capture the initial schema, the first time Maxwell is run.\n\n\nAs time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the \nschemas\n table - each diff contains the following information to place it in the timeline of your database:\n\n\n\n\nbinlog_file\n, \nbinlog_position\n (or \ngtid_set\n): the exact point in the binlog stream where the schema change occurred\n\n\ndeltas\n: the internal representation of the schema change\n\n\nbase_schema_id\n: the previous schema that this delta applies to\n\n\nlast_heartbeat_read\n: the most recent maxwell heartbeat seen in the binlog prior to this change\n\n\nserver_id\n: the server which this schema applies to\n\n\n\n\nThis information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of \nbase_schema_id\n until it terminates (in a \nnull\n, which means we've reached the initial captured schema).\n\n\n\"Most recent\" should be fairly intuitive - firstly we sort by \nlast_heartbeat_read\n, then by \nbinlog_file\n, then \nbinlog_position\n. We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position.\n\n\nMaster failover\n\n\n\n\nSchemas are important for master failover. When Maxwell detects that it is talking to a new \nserver_id\n (one that differs from its stored \nposition\n), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to \nmaxwell.heartbeats\n corresponding to the timestamp stored in its position table.\n\n\nOnce it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event.\n\n\nUsing this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new \nserver_id\n, and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.", 
            "title": "Schemas"
        }, 
        {
            "location": "/schemas/#schema-storage", 
            "text": "This document describes Maxwell's internal schema storage mechanism. Most users won't need to know how this works, but it can be useful in the rare case where something goes wrong.  MySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc.  There is a set of \"base schema\" tables in the maxwell database -  tables ,  columns ,  databases . This is where we capture the initial schema, the first time Maxwell is run.  As time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the  schemas  table - each diff contains the following information to place it in the timeline of your database:   binlog_file ,  binlog_position  (or  gtid_set ): the exact point in the binlog stream where the schema change occurred  deltas : the internal representation of the schema change  base_schema_id : the previous schema that this delta applies to  last_heartbeat_read : the most recent maxwell heartbeat seen in the binlog prior to this change  server_id : the server which this schema applies to   This information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of  base_schema_id  until it terminates (in a  null , which means we've reached the initial captured schema).  \"Most recent\" should be fairly intuitive - firstly we sort by  last_heartbeat_read , then by  binlog_file , then  binlog_position . We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position.", 
            "title": "Schema storage"
        }, 
        {
            "location": "/schemas/#master-failover", 
            "text": "Schemas are important for master failover. When Maxwell detects that it is talking to a new  server_id  (one that differs from its stored  position ), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to  maxwell.heartbeats  corresponding to the timestamp stored in its position table.  Once it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event.  Using this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new  server_id , and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.", 
            "title": "Master failover"
        }, 
        {
            "location": "/compat/", 
            "text": "Requirements:\n\n\n\n\n\n\nJRE 7 or above\n\n\nmysql 5.1, 5.5, 5.6, 5.7, 8\n\n\nkafka 0.8.2 or greater\n\n\n\n\nbinlog_row_image=MINIMAL\n\n\n\n\nAs of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:\n\n\n\n\nINSERT statements will no longer output a column's default value\n\n\nUPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but \ndata\n will only include what is needed to perform the update (generally, id columns and changed columns).\n  The \nold\n section may or may not be included, depending on the nature of the update.\n\n\nDELETE statements will be incomplete; generally they will only include the primary key.\n\n\n\n\nMaster recovery\n\n\n\n\nAs of 1.2.0, maxwell includes experimental support for master position recovery.  It works like this:\n\n\n\n\nmaxwell writes heartbeats into the binlogs (via the \npositions\n table)\n\n\nmaxwell reads its own heartbeats, using them as a secondary position guide\n\n\nif maxwell boots and can't find its position matching the \nserver_id\n it's\n  connecting to, it will look for a row in \nmaxwell.positions\n from a different\n  server_id.\n\n\nif it finds that row, it will scan backwards in the binary logs of the new\n  master until it finds that heartbeat.\n\n\n\n\nNotes:\n\n\n\n\nmaster recovery is not compatible with separate schema-store hosts and\n  replication-hosts, due to the heartbeat mechanism.\n\n\nthis code should be considered alpha-quality.\n\n\non highly active servers, as much as 1 second of data may be duplicated.\n\n\nmaster recovery is not available in GTID-mode.\n\n\n\n\nMySQL binlog connector\n\n\n\n\nAs of 1.11.0, maxwell uses \nshyiko/mysql-binlog-connector-java\n as its underlying\nreplication library (previously it was opt-in via \n--binlog_connector\n). This is\nlargely compatible with the previous OpenReplicator implementation, but there are some differences:\n\n\n\n\nTIMESTAMP columns are always treated as UTC, regardless of your timezone. See\n   \nissue #681\n for more details.", 
            "title": "Compat / Caveats"
        }, 
        {
            "location": "/compat/#requirements", 
            "text": "JRE 7 or above  mysql 5.1, 5.5, 5.6, 5.7, 8  kafka 0.8.2 or greater", 
            "title": "Requirements:"
        }, 
        {
            "location": "/compat/#binlog_row_imageminimal", 
            "text": "As of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:   INSERT statements will no longer output a column's default value  UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but  data  will only include what is needed to perform the update (generally, id columns and changed columns).\n  The  old  section may or may not be included, depending on the nature of the update.  DELETE statements will be incomplete; generally they will only include the primary key.", 
            "title": "binlog_row_image=MINIMAL"
        }, 
        {
            "location": "/compat/#master-recovery", 
            "text": "As of 1.2.0, maxwell includes experimental support for master position recovery.  It works like this:   maxwell writes heartbeats into the binlogs (via the  positions  table)  maxwell reads its own heartbeats, using them as a secondary position guide  if maxwell boots and can't find its position matching the  server_id  it's\n  connecting to, it will look for a row in  maxwell.positions  from a different\n  server_id.  if it finds that row, it will scan backwards in the binary logs of the new\n  master until it finds that heartbeat.   Notes:   master recovery is not compatible with separate schema-store hosts and\n  replication-hosts, due to the heartbeat mechanism.  this code should be considered alpha-quality.  on highly active servers, as much as 1 second of data may be duplicated.  master recovery is not available in GTID-mode.", 
            "title": "Master recovery"
        }, 
        {
            "location": "/compat/#mysql-binlog-connector", 
            "text": "As of 1.11.0, maxwell uses  shyiko/mysql-binlog-connector-java  as its underlying\nreplication library (previously it was opt-in via  --binlog_connector ). This is\nlargely compatible with the previous OpenReplicator implementation, but there are some differences:   TIMESTAMP columns are always treated as UTC, regardless of your timezone. See\n    issue #681  for more details.", 
            "title": "MySQL binlog connector"
        }, 
        {
            "location": "/changelog/", 
            "text": "Maxwell changelog\n\n\nv1.22.1\n: \"a snow covered field\"\n\n\n\n\nfix crash in rabbit-mq producer\n\n\nbetter support for maxwell + azure-mysql\n\n\nremove bogus different-host bootstrap check\n\n\nsome security upgrades\n\n\n\n\nv1.22.0\n: \"through the roof, and underground\"\n\n\n\n\nBootstrapping has been reworked and is now available in all setups,\nincluding those in which the maxwell store is split from the replicator.\n\n\ncleanup and fix a deadlock in the kafka fallback queue logic\n\n\nadd .partition_string = to javascript filters\n\n\n\n\nv1.21.1\n: \"ohhhhhh oh oh\"\n\n\n\n\nUpgrade binlog connector.  Should fix issues around deserialization\nerrors.\n\n\n\n\nv1.21.0\n: \"through the roof\"\n\n\n\n\nBootstrapping output no longer contain binlog positions.  Please update\n  any code that relies on this.\n\n\nFix 3 parser issues.\n\n\n\n\nv1.20.0\n: \"and so you learn the only way to go is\"\n\n\n\n\nadd support for partitioning by transaction ID thx @hexene\n\n\nadd support for a kafka \"fallback\" topic to write to\n  when a message fails to write\n\n\nadd UJIS charset support\n\n\nparser bug: multiple strings concatenate to make one default string\n\n\nparser bug: deal with bizarre column renames which are then referenced\n  in AFTER column statements\n\n\n\n\nv1.19.7\n: \"in every corner of your room\"\n\n\n\n\nfix a parser error with empty sql comments\n\n\ninterpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei\n\n\n\n\nv1.19.6\n: \"set up for you\"\n\n\n\n\nFurther fixes for GTID-reconnection issues.\n\n\nCrash sanely when GTID-enabled maxwell is connected to clearly the wrong master,\n  thanks @acampoh\n\n\n\n\nv1.19.5\n: \"when there is trap\"\n\n\n\n\nFixes for unreliable connections wrt to GTID events; previously we\n  restart in any old position, now we throw away the current transaction\n  and restart the replicator again at the head of the GTID event.\n\n\n\n\nv1.19.4\n: \"and underground\"\n\n\n\n\nFixes for a maxwell database not making it through the blacklist\n\n\nAdd \noutput_null_zerodates\n parameter to control how we treat\n  '0000-00-00'\n\n\n\n\nv1.19.3\n: \"through the roof\"\n\n\n\n\nAdd a universal backpressure mechanism.  This should help people who\nwere running into out-of-memory situations while bootstrapping.\n\n\n\n\nv1.19.2\n: \"the same I wore last night\"\n\n\n\n\nInclude schema_id in bootstrap events\n\n\nadd more logging around binlog connector losing connection\n\n\nadd retry logic to redis\n\n\nsome aws fixes\n\n\n\n\nallow pushing JS hashes/arrays into data from js filters\n\n\n\n\n\n\nlist changes\n\n\n\n\n\n\nv1.19.1\n: \"the swoop here doesn't change things one bit\"\n\n\n\n\nHandle mysql bit literals in DEFAULT statements\n\n\nblacklist out CREATE ROLE etc\n\n\nupgrade dependencies to pick up security issues\n\n\n\n\nv1.19.0\n: \"whole lotta milka\"\n\n\n\n\nmysql 8 support!\n\n\nutf8 enum values are supported now\n\n\nfix #1125, bootstrapping issue for TINYINT(1)\n\n\nfix #1145, nasty bug around SQL blacklist and columns starting with \"begin\"\n\n\nonly resume bootstraps that are targeted at this client_id\n\n\nfixes for blacklists and heartbeats.  Did I ever mention blacklists\n  are a terrible idea?\n\n\n\n\nv1.18.0\n: \"hello from the Andes\"\n\n\n\n\nmemory optimizations for large schemas (especially shareded schemas with lots of duplicates)\n\n\nadd support for an http endpoint to support Prometheus metrics\n\n\nallow javascript filters to access the row query object\n\n\njavascript filters now run in the bootstrap process\n\n\nsupport for non-latin1 column names\n\n\nadd \n--output_schema_id\n option\n\n\nbetter handling of packet-too-big errors from Kinesis\n\n\nadd message.publish.age metric\n\n\n\n\nv1.17.1\n: \"ay, ay, ay\"\n\n\n\n\nfix a regression around filters + bootstrapping\n\n\nfix a regression around filters + database-only-ddl\n\n\n\n\nv1.17.0\n: \"monday, not sunday tuesday\"\n\n\nv1.17.0 brings a new level of configurability by allowing you to inject\na bit of javascript into maxwell's processing.  Should be useful!  Also:\n\n\n\n\nfix regression for Alibaba RDS tables\n\n\n\n\nv1.16.1\n: \"the 90 degree angle thing\"\n\n\n\n\nFix Bootstrapping for JSON columns\n\n\nadd --recapture_schema flag for when ya wanna start over\n\n\nadd kafka 1.0 libraries, make them default\n\n\n\n\nv1.16.0\n: \"kind of sort of a reference to something\"\n\n\nv1.16.0 brings a rewrite of Maxwell's filtering system, giving it a\nconcise list of rules that are executed in sequence.  It's now possible\nto exclude tables from a particular database, exclude columns matching a\nvalue, and probably some other use cases.\nSee http://maxwells-daemon.io/config/#filtering for details.\n\n\nv1.15.0\n: \"I'm sure I'm being supportive here.\"\n\n\nThis is a bug-fix release, but it's big enough I'm giving it a minor\nversion.\n\n\n\n\nFix a very old bug in which DDL rows were writing the \nstart\n of the\nrow into \nmaxwell.positions\n, leading to chaos in some scenarios where\nmaxwell managed to stop on the row and double-process it, as well as to\na few well-meaning patches.\n\n\nFix the fact that maxwell was outputting \"next-position\" instead of\n\"position\" of a row into JSON.\n\n\nFix the master-recovery code to store schema that corresponds to the\nstart of a row, and points the replicator at the next-position.\n\n\n\n\nMuch thanks to Tim, Likun and others in sorting this mess out.\n\n\nv1.14.7\n: \"casamir pulaski day\"\n\n\n\n\nadd RowMap#getRowQuery, thx @saimon7\n\n\nrevert alpine-linux docker image fiasco\n\n\nfix RawJSONString not serializable, thx @niuhaifeng\n\n\n\n\nv1.14.6\n: \"gimme one sec, I need to grab something\"\n\n\n\n\nFix docker image\n\n\n\n\nv1.14.5\n: \"he looks funny, he moves funny\"\n\n\n\n\nreduce docker image footprint\n\n\nadd benchmarking framework\n\n\nperformance improvements for date/datetime columns\n\n\nfix parser error on UPGRADE PARTITIONING\n\n\n\n\nv1.14.4\n: \"chinese food\"\n\n\n\n\nFix race condition in SchemaCapturer\n\n\n\n\nv1.14.3\n: \"what's for lunch?\"\n\n\n\n\nEnable jvm metrics\n\n\n\n\nv1.14.2\n: \"bork bork bork\"\n\n\n\n\nfix regression in 1.14.1 around bootstrapping host detection\n\n\nfix heartbeating code around table includes\n\n\n\n\nv1.14.1\n: \"half asleep in frog pajamas\"\n\n\n\n\nbootstraps can now take a client_id\n\n\nimproved config validation for embedded mode\n\n\n\n\nv1.14.0\n: \"cats, cats, more cats.  sadness at lack of cats.\"\n\n\n\n\nnew feature \n--output_xoffset\n to uniquely identify rows within transactions,\n  thx Jens Gyti\n\n\nBug fixes around \"0000-00-00\" times.\n\n\nBug fixes around dates pre 1000 AD\n\n\n\n\nv1.13.5\n: \"cyclone keni is real\"\n\n\n\n\nSupport environment variable based configuration\n\n\n\n\nv1.13.4\n: \"it was just a dream\"\n\n\n\n\nAdded possibility to do not declare the rabbitmq exchange.\n\n\n\n\nv1.13.3\n: \"winner winner chicken dinner\"\n\n\n\n\nAdd logging for binlog errors\n\n\nMaven warning fix\n\n\nDo not include current position DDL schema to avoid processing DDL twice\n\n\nAlways write null fields in primary key fields\n\n\nBugfix: fix http_path_prefix command line option issue\n\n\n\n\nv1.13.2\n: \"I just bought them to sleep in\"\n\n\n\n\nfix a bug with CHARACTER SET = DEFAULT\n\n\nmaxwell now eclipse-friendly.\n\n\nconfigurable bind-address for maxwell's http server\n\n\n\n\nv1.13.1\n: \"line up your exes in song\"\n\n\n\n\nredis producer now supports LPUSH, thx @m-denton\n\n\nRowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti\n\n\nbugfix: fix jdbc option parsing when value contains \n=\n\n\nbugfix: apparently the SQS producer was disabled\n\n\nbugfix: fix a situation where adding a second client could cause\n  schemas to become out of sync\n\n\nsupport for --daemon\n\n\n\n\nv1.13.0\n: \"sorry, I burned your clothes\"\n\n\n\n\nproper SSL connection support, thanks @cadams5\n\n\nsupport for including original SQL in insert/update/deletes, thanks @saimon7\n\n\nfixes for float4, float8 and other non-mysql datatypes\n\n\nbump kinesis lib to 0.12.8\n\n\nfix for bug when two databases share a single table\n\n\n\n\nv1.12.0\n: \"Cold Feet, literally and metaphorically.\"\n\n\n\n\nSupport for injecting a custom producer, thanks @tomcollinsproject\n\n\nNew producer for Amazon SQS, thanks @vikrant2mahajan\n\n\nMaxwell can now filter rows based on column values, thanks @finnplay\n\n\nFixes for the Google Pubsub producer (it was really broken), thanks @finnplay\n\n\nDDL output can now optionally include the source SQL, thanks @sungjuly\n\n\nSupport for double-quoted table/database/etc names\n\n\nrabbitmq option for persistent messages, thanks @d-babiak\n\n\nSQL parser bugfix for values like +1.234, thanks @hexene\n\n\n\n\nv1.11.0\n: \"the latest, the greatest\"\n\n\n - default kafka client upgrades to 0.11.0.1\n - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803)\n\n\n\nv1.10.9\n: \"no one left behind\"\n\n\nWe recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9.\n\n\n\n\nAdd missing Kafka clients\n\n\nListen and report on binlog connector lifecycle events for better visibility\n\n\nReduce docker image size\n\n\n\n\nv1.10.8\n: \"what doesn't kill you makes you stronger\"\n\n\n\n\nFix docker builds\n\n\nAdd Google Cloud Pub/Sub producer\n\n\nRabbitMQ producer enhancements\n\n\n\n\nv1.10.7\n: \"it's never too l8!\"\n\n\n\n\nJava 8 upgrade\n\n\nDiagnostic health check endpoint\n\n\nEncryption\n\n\nDocumentation update: encryption, kinesis producer, schema storage fundamentals, etc.\n\n\n\n\nv1.10.6\n: \"a new starter is here\"\n\n\n\n\nBinlog-connector upgrade\n\n\nBug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special\n\n\nIf master recovery is enabled, Maxwell cleans up old positions for the same server and client id\n\n\n\n\nv1.10.5\n: \"half asleep on her couch\"\n\n\n\n\nShyko's binlog-connector is now the default and only replication\nbackend available for maxwell.\n\n\n\n\nv1.10.4\n: \"shutdown --harder\"\n\n\nNotable changes:\n\n\n\n\nShutdown hardening. If maxwell can't shut down (because the kafka\n   producer is in a bad state and \nclose()\n never terminates, for example),\n   it would previously stall and process no messages. Now, shutdown is run\n   in a separate thread and there is an additional watchdog thread which\n   forcibly kills the maxwell process if it can't shut down within 10\n   seconds.\n\n\nInitial support for running maxwell from java, rather then as its own\n   process. This mode of operation is still experimental, but we'll\n   accept PRs to improve it (thanks Geoff Lywood).\n\n\nFix incorrect handling of negative (pre-epoch dates) when using\n   binlog_connector mode (thanks Geoff Lywood).\n\n\n\n\nv1.10.3\n: \"1.10.2-and-a-bit\"\n\n\n\n\ntiny release to fix a units error in the \nreplication.lag\n metric\n   (subtracting seconds from milliseconds)\n\n\n\n\nv1.10.2\n: \"just in time for tomorrow\"\n\n\n\n\nadded metrics: \"replication.queue.time\" and \"inflightmessages.count\"\n\n\nrenamed \"time.overall\" metric to \"message.publish.time\"\n\n\ndocumentation updates (thanks Chintan Tank)\n\n\n\n\nv1.10.1\n: \"forgive and forget\"\n\n\nThe observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes:   \n\n\n\n\nSupport Kafka 0.10.2   \n\n\nStop procesing RDS hearbeats   \n\n\nKeep maxwell heartbeat going every 10 seconds when database is quiet   \n\n\nAllow for empty double-quoted string literals for database schema changes   \n\n\nIgnore Kafka/Kinesis producer errors based on new configuration ignore_producer_error\n\n\n\n\nv1.10.0\n: \"slightly more ones than zeroes\"\n\n\nThis is a small release, primarily around a change to how schemas are\nstored. Maxwell now stores the \nlast_heartbeat_read\n with each entry\nin the \nschemas\n table, making schema management more resilient to\ncases where binlog numbers are reused, but means that you must take\ncare if you need to roll back to an earlier version. If you deploy\nv1.10.0, then roll back to an earlier version, you should manually\nupdate all \nschemas\n.\nlast_heartbeat_read\n values to \n0\n before\nredeploying v1.10.0 or higher.\n\n\nOther minor changes:\n\n\n\n\nallow negative default numbers in columns\n\n\nonly store final binlog position if it has changed\n\n\nblacklist internal aurora table `rds_heartbeat*'\n\n\nlog4j version bump (allows for one entry per line JSON logging)\n\n\n\n\nv1.9.0\n: \"now with added whimsy\"\n\n\nMaxwell 1.9 adds one main feature: monitoring support, contributed by\nScott Ferguson. Multiple backends can be configured, read the updated\ndocs for full details.\n\n\nThere's also some bugfixes:\n\n\n\n\nfilter DDL messages based on config\n\n\ndetermine newest schema from binlog order, not creation order\n\n\nadd task manager to shutdown cleanly on error\n\n\nminor logging improvements\n\n\n\n\nv1.8.2\n: \"just as the postcards wept\"\n\n\nBugfix release.\n\n\n\n\nmaxwell would crash on a quoted partition name\n\n\nfixes for alters on non-string tables containing VARCHAR\n\n\nuse seconds instead of milliseconds for DDL messages\n\n\n\n\nv1.8.1\n: \"famous is faster, don't have to be talented\"\n\n\n\n\nperformance improves in capturing and restoring schema, thx Joren\n  Minnaert\n\n\nAllow for capturing from a separate mysql host (adds support for using\n  Maxscale as a replication proxy), thx Adam Szkoda\n\n\n\n\nv1.8.0\n: \"upbeat, honest, contradictory\"\n\n\nIn version 1.8.0 Maxwell gains alpha support for GTID-based positions!\nAll praise due to Henry Cai.\n\n\nv1.7.2\n: \"comparing self to better\"\n\n\n\n\nFix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a\n  binlog event, leading to crashes or in some cases data mismatches.\n\n\n\n\nv1.7.1\n: \"blame it on your seratonin\"\n\n\n\n\nbootstrapping now can take a \n--where\n clause\n\n\nperformance improvements in the kafka producer\n\n\n\n\nv1.7.0\n: \"lucky me, lucky mud\"\n\n\nMaxwell 1.7 brings 2 major new, alpha features.  The first is Mysql 5.7\nsupport, including JSON column type support and handling of 5.7 SQL, but\n\nnot\n including GTID support yet.  This is based on porting Maxwell to\nStanley Shyko's binlog-connector library.  Thanks to Stanley for his\namazing support doing this port.\n\n\nThe second major new feature is a producer for Amazon's Kinesis streams,\nThis was contributed in full by the dogged and persistent Thomas Dziedzic.\nCheck it out with \n--producer=kinesis\n.\n\n\nThere's also some bugfixes:\n- Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson\n- allow CHECK() statements inside column definitions\n\n\nv1.6.0\n: \"give me a quest\"\n\n\nThis is mostly a bugfix release, but it gets a minor version bump due to\na single change of behavior: dates and timestamps which mysql may\naccept, but are considered invalid (0000-00-00 is a notable example)\npreviously had inconsistent behavior.  Now we convert these to NULL.\nOther bugfixes:\n- heartbeats have moved into their own table\n- more fixes around alibaba rds\n- ignore DELETE statements that are output for MEMORY tables upon server\n  restart\n- allow pointing maxwell to a pre-existing database\n\n\nv1.5.2\n: \"french banana\"\n\n\n\n\nadd support for kafka 0.10.1 @ smferguson\n\n\nmaster recovery: cleanup positions from previous master; prevent\n  errors on flip-back.\n\n\nfix a bug that would trigger in certain cases when dropping a column\n  that was part of the primary-key\n\n\n\n\nv1.5.1\n: \"1.5.1 is just 1.5.1\"\n\n\nThis is a bugfix release.\n- fixes for bootstrapping with an alternative maxwell-schema name and an\n  \ninclude_database\n filter, thanks Lucian Jones\n- fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson\n- ignore the RDS table \nmysql.ha_health_check\n table\n- Get the bootstrapping process to output NULL values.\n- fix a quoting issue in the bootstrap code, thanks @mylesjao.\n\n\nv1.5.0\n: \"someone, somewhere, is still smoking cigarettes, damnit\"\n\n\n\n\nCHANGE: Kafka producer no longer ships with hard-coded defaults.\n  Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\"\n  configured to your liking.\n\n\nbugfix: fix a regression in handling \nALTER TABLE change c int after b\n statements\n\n\nwarn on servers with missing server_id\n\n\n\n\nv1.4.2\n: \"drawer cat is back\"\n\n\n\n\nkafka 0.10.0 support, as well as a re-working of the --kafka_version\n  command line option.\n\n\n\n\nv1.4.1\n: \"cat snores\"\n\n\n\n\nsupport per-table topics, Thanks @smferguson and @sschatts.\n\n\nfix a parser issue with DROP COLUMN CASCADE, thanks @smferguson\n\n\n\n\nv1.4.0\n: \"deep, insomniac character flaws\"\n\n\n1.4.0 brings us two nice new features:\n- partition-by-column: see --kafka_partition_columns.  Thanks @smferguson\n- output schema changes as JSON: see --output_ddl.  Thanks @xmlking\n- As well as a fix around race conditions on shutdown.\n\n\nv1.3.0\n: \"yogg-saron\"\n\n\n\n\nsupport for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan\n\n\nsupport for outputting server_id \n thread_id, thanks @sagiba\n\n\nfix a race condition in bootstrap support\n\n\n\n\nv1.2.2\n: \"bats wearing frog pajamas\"\n\n\n\n\nMaxwell will now include by default fields with NULL values (as null\n  fields).  To disable this and restore the old functionality where fields\n  were omitted, pass \n--output_nulls=false\n\n\nFix an issue with multi-client support where two replicators would\n  ping-pong heartbeats at each other\n\n\nFix an issue where a client would attempt to recover a position from a\n  mismatched client_id\n\n\nFix a bug when using CHANGE COLUMN on a primary key\n\n\n\n\nv1.2.1\n: \"point-ones are a sad and inevitable fact\"\n\n\nThis is a bugfix release.\n- fix a parser bug around ALTER TABLE CHARACTER SET\n- fix bin/maxwell to pull in the proper version of the kafka-clients\n  library\n\n\nv1.2.0\n: \"just here, not to talk to you\"\n\n\n1.2.0 is a major release of Maxwell that introduces master recovery\nfeatures; when a slave is promoted to master, Maxwell is now capable of\nrecovering the position.  See the \n--master_recovery\n flag for more\ndetails.\n\n\nIt also upgrades the kafka producer library to 0.9.  If you're using\nmaxwell with a kafka 0.8 server, you must now pass the \n--kafka0.8\n flag\nto maxwell.\n\n\nv1.1.6\n: \"pithy\"\n\n\n\n\nminor bugfix in which maxwell with --replay mode was trying to write\n  heartbeats\n\n\n\n\nv1.1.5\n: \"my brain is a polluted mess\"\n\n\n\n\n@dadah89 adds --output_binlog_position to optionally output the\n  position with the row\n\n\n@dadah89 adds --output_commit_info to turn off xid/commit fields\n\n\nmaxwell now supports tables with partitions\n\n\nmaxwell now supports N maxwells per-server.  see the client_id /\n  replica_server_id options.\n\n\ntwo parser fixes, for engine=\ninnodb\n and CHARSET ASCII\n\n\nlay the ground work for doing master recovery; we add a heartbeat into\n  the positions table that we can co-ordinate around.\n\n\n\n\nv1.1.4\n: \"george flunk\"\n\n\n\n\nadd support for a bunch more charsets (gbk, big5, notably)\n\n\nfix Maxwell's handling of kafka errors - previously we were trying to\n  crash Maxwell by throwing a RuntimeException out of the Kafka\n  Producer, but this was a failure.  Now we log and skip all errors.\n\n\n\n\nv1.1.3\n: \"the button I push to not have to go out\"\n\n\nThis is a bugfix release, which fixes:\n- https://github.com/zendesk/maxwell/issues/376, a problem parsing\n  RENAME INDEX\n- https://github.com/zendesk/maxwell/issues/371, a problem with the\n  SERIAL datatype\n- https://github.com/zendesk/maxwell/issues/362, we now preserve the\n  original casing of columns\n- https://github.com/zendesk/maxwell/issues/373, we were incorrectly\n  expecting heartbeats to work under 5.1\n\n\nv1.1.2\n: \"scribbled notes on red pages\"\n\n\n\n\npick up latest mysql-connector-j, fixes #369\n\n\nfix an issue where maxwell could skip ahead positions if a leader failed.\n\n\nrework buffering code to be much kinder to the GC and JVM heap in case\n  of very large transactions / rows inside transactions\n\n\nkinder, gentler help text when you specify an option incorrectly\n\n\n\n\nv1.1.1\n: scribbled notes on blue pages\n\n\n\n\nfixes a race condition setting the binlog position that would get\n  maxwell stuck\n\n\n\n\nv1.1.0\n: \"sleep away the afternoon\"\n\n\n\n\nmuch more efficient processing of schema updates storage, especially when dealing with large schemas.\n\n\n@lileeyao added --exclude-columns and the --jdbc_options features\n\n\n@lileeyao added --jdbc_options\n\n\ncan now blacklist entire databases\n\n\nnew kafka key format available, using a JSON array instead of an object\n\n\nbugfix: unsigned integer columns were captured incorrectly.  1.1 will\n  recapture the schema and attempt to correct the error.\n\n\n\n\nv1.1.0-pre4\n: \"buck buck buck buck buck buck-AH!\"\n\n\n\n\nEddie McLean gives some helpful patches around bootstrapping\n\n\nBugfixes for the patch-up-the-schema code around unsigned ints\n\n\n\n\nv1.1.0-pre3\n:\n\n\n\n\nforgot to include some updates that back-patch unsigned column\n  problems\n\n\n\n\nv1.1.0-pre2\n: \"yawn yawn\"\n\n\n\n\nfix performance issues when capturing schema in AWS Aurora\n\n\nfix a bug in capturing unsigned integer columns\n\n\n\n\nv1.0.1\n: \"bag of oversized daisies\"\n\n\n\n\nfixes a parsing bug with \nCURRENT_TIMESTAMP()\n\n\n\n\nv1.0.0\n: \"Maxwell learns to speak\"\n\n\nSince v0.17.0, Maxwell has gotten:\n- bootstrapping support\n- blacklisting for tables\n- flexible kafka partitioning\n- replication heartbeats\n- GEOMETRY columns\n- a whole lotta lotta bugfixes\n\n\nand I, Osheroff, think the damn thing is stable enough for a 1.0.  So\nthere.\n\n\nv1.0.0-RC3\n: \"C'mon and take it\"\n\n\npull in support for replication heartbeats.  helps in the flakier\nnetwork environs.\n\n\nv1.0.0-RC2\n: \"same thing, just without the v\"\n\n\n\n\nfixes the way ALTER DATABASE charset= was handled\n\n\nadds proper handling of ALTER TABLE CONVERT TO CHARSET\n\n\n\n\nv1.0.0-RC1\n: \"Richard Buckner's release\"\n\n\n\n\nmodifications to the way the bootstrap utility works\n\n\nfix a race condition crash bug in bootstrapping\n\n\nfix a parser bug\n\n\n\n\nv1.0.0-PRE2\n: \"an embarassment of riches\"\n\n\n1.0.0-PRE2 brings in a lot of changes that got merged while we were\ntesting out PRE1.  so, hey.\n- Configurable names for the \nmaxwell\n schema database (Kristian Kaufman)\n- Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman)\n- Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman)\n- support GEOMETRY columns, output as well-known-text\n- add \n--blacklist_tables\n option to fully ignore excessive schema changes (Nicolas Maquet)\n- bootstrap rows now have 'bootstrap-insert' type\n\n\nv1.0.0-PRE1\n: \"drunk conversations with sober people\"\n\n\n\n\nHere we have the preview release of @nmaquet's excellent work around\n  bootstrapping initial versions of mysql tables.\n\n\n\n\nv0.17.0\n: \"wrists of William\"\n\n\nv0.17 is a large bugfix release with one new feature.\n- FEATURE: allow specifying an alternative mysql schema-storage server and\n  replication server\n- BUGFIX: properly handle case-sensitivity by aping the behavior of the\n  master server.  Fixes #230.\n- BUGFIX: parse some forms of CHECK( ... ) statements.  Fixes #203.\n- BUGFIX: many more SQL-parser fixes.  We are mostly through some\n  thousands of lines of SQL produced by mysql-test.\n\n\nv0.16.2\n: \"The best laid plans\"\n\n\nThis is a large-ish bugfix release.\n- Support, with reservations, binlog_row_image=MINIMAL\n- parser bug: handle table names that look like floating points\n- parser bug: fix for entity names that have '.', '\\', etc in them\n- handle UPPERCASE encoding names\n- support UCS2 (start trying to operate ok on the mysql-test suite)\n- use ObjectOutputStream.reset to fix memory leaks when buffering to disk\n\n\nv0.16.1\n: \"me and room service\"\n\n\nThis is a bug-fix-roundup release:\n- support ALTER DATABASE\n- fix a bunch of parse errors: we've started running mysql-test at\n  maxwell and are fixing up failures.\n- some modifications to the overflow-to-disk logic; we buffer the input\n  and output, and we fix a memory leak\n\n\nv0.16.0\n: \"Kristian Kaufmann's version\"\n\n\nVersion 0.16.0 introduces a feature where UPDATE statements will now\nshow both the new row image and the old values of the fields that\nchanged.  Thanks @kristiankaufmann\n\n\nv0.15.0\n: \"the littlest little city\"\n\n\n\n\nfix a parse problem with indices ordered by ASC/DESC\n\n\n\n\nv0.15.0-RC1\n: \"it's later than you think\"\n\n\n\n\nlarge transactions now buffer to disk instead of crushing maxwell.\n\n\nsupport ALGORITHM=[algo], LOCK=[lock] for 5.6 alters\n\n\n\n\nv0.14.6\n: \"It's about being American.  Sort of.\"\n\n\n\n\nfix TIME column support\n\n\nfix parsing on millisecond precision column defintions\n\n\nfix CREATE SCHEMA parsing\n\n\n\n\nv0.14.5\n: \"false is the new true\"\n\n\n\n\nhandle BOOLEAN columns with true/false defaults\n\n\n\n\nv0.14.4\n: \"You'd think we'd be at 1.0 by now, wouldn't you?\"\n\n\n\n\nfixes parsing of \"mysql comments\" (\n/*! .. */\n)\n\n\nMore performance improvements, another 10% in a tight loop.\n\n\n\n\nv0.14.3\n: \"Peanuts.  My girlfriend thinks about peanuts.\"\n\n\n\n\nfixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.\n\n\n\n\nv0.14.2\n: \"Maxwell Sandvik 88\"\n\n\n\n\ncapture the mysql database along with the rest of the schema.  Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. \n\n\n\n\nv0.14.1\n: \"be liberal in what you accept.  Even if nonsensical.\"\n\n\n\n\nfixes a parser bug around named PRIMARY KEYs.\n\n\n\n\nv0.14.0\n: \"the slow but inevitable slide\"\n\n\nThis release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.  \n\n\nv0.13.1\n: \"well that was somewhat expected\"\n\n\nv0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. \n\n\nv0.13.0 contains:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around \nCREATE TABLE ... IF NOT EXISTS\n, which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n\nv0.13.0\n: \"Malkovich Malkovich Malkovich Sheldon?\"\n\n\nLucky release number 13 brings some reasonably big changes:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around \nCREATE TABLE ... IF NOT EXISTS\n, which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n\nThis release has a pretty bad bug.  do not use.\n\n\nv0.12.0\n: \"what do I call them?  Slippers?  Why, are you jealous?\"\n\n\n\n\nadd support for BIT columns.  \n\n\n\n\nv0.11.4\n: \"13 steps\"\n\n\nthis is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event.  I really need to fix this at a lower level, ie the open-replicator level.\n\n\nv0.11.3\n: \".. and the other half is to take the bugs out\"\n\n\nthis is a bugfix release:\n- fix problems with table creation options inside alter statements ( \nALTER TABLE foo auto_increment=10\n )\n- fix a host of shutdown-procedure bugs\n\n\nthe test suite should also be way more reliable, not like you care.\n\n\nv0.11.2\n: \"savage acts of unprovoked violence are bad\"\n\n\nThis is a bugfix release.  It includes:\n- soft deletions of maxwell.schemas to fix A-\nB-\nA master swapping without creating intense replication delay\n- detect and fail early if we see \nbinlog_row_image=minimal\n\n- kill off maxwell if the position thread dies\n- fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)\n\n\nv0.11.1\n: \"dog snoring loudly\"\n\n\n\n\nmaxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid.  this should prevent the worst of cases.\n\n\n\n\nv0.11.0\n: \"cat waving gently\"\n\n\nThis release of Maxwell preserves transaction information in the kafka stream by adding a \nxid\n key in the JSON object, as well as a \ncommit\n key for the final row inside the transaction.\n\n\nIt also contains a bugfix around server_id handling.\n\n\nv0.10.1\n: \"all 64 of your bases belong to... shut up, internet parrot.\"\n\n\n\n\nproper support for BLOB, BINARY, VARBINARY columns (base 64 encoded)\n\n\nfix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash\n\n\nmake table option parsing more lenient\n\n\n\n\nv0.11.0-RC1\n: \"goin' faster than a rollercoaster\"\n\n\n\n\nmerge master fixes\n\n\n\n\nv0.10.0\n: \"The first word is French\"\n\n\n\n\nMysql 5.6 checksum support!\n\n\nsome more bugfixes with the SQL parser \n\n\n\n\nv0.11.0-PRE4\n: \"except for that other thing\"\n\n\n\n\nbugfix on v0.11.0-PRE3\n\n\n\n\nv0.11.0-PRE3\n: \"nothing like a good night's sleep\"\n\n\n\n\nhandle SAVEPOINT within transactions\n\n\ndowngrade unhandled SQL to a warning\n\n\n\n\nv0.11.0-PRE2\n: \"you really need to name a \nPRE\n release something cutesy?\"\n\n\n\n\nfixes for myISAM \"transactions\"\n\n\n\n\nv0.11.0-PRE1\n: \"A slow traffic jam towards the void\"\n\n\n\n\nfix a server_id bug (was always 1 in maxwell.schemas)\n\n\nJSON output now includes transaction IDs\n\n\n\n\nv0.10.0-RC4\n: \"Inspiring confidence\"\n\n\n\n\ndeal with BINARY flag in string column creation.\n\n\n\n\nv0.9.5\n: \"Long story short, that's why I'm late\"\n\n\n\n\nhandle the BINARY flag in column creation\n\n\n\n\nv0.10.0-RC3\n: \"Except for that one thing\"\n\n\n\n\nhandle \"TRUNCATE [TABLE_NAME]\" statements\n\n\n\n\nv0.10.0-RC2\n: \"RC2 is always a good sign.\"\n\n\n\n\nfixes a bug with checksum processing.\n\n\n\n\nv0.10.0-RC1\n: \"verify all the things\"\n\n\n\n\nupgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.\n\n\n\n\nv0.9.4\n: \"we've been here before\"\n\n\n\n\nallow a configurable number (including unlimited) of schemas to be stored\n\n\n\n\nv0.9.3\n: \"some days it's just better to stay in bed\"\n\n\n\n\nbump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes\n\n\n\n\nv0.9.2\n: \"Cat's tongue\"\n\n\n\n\nbump open-replicator buffer to 50mb by default\n\n\nlog to STDERR, not STDOUT \n\n\n--output_file\n option for file producer\n\n\n\n\nv0.9.1\n: \"bugs, bugs, bugs, lies, statistics\"\n\n\n\n\nMaxwell is now aware that column names are case-insenstive\n\n\nfix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.\n\n\n\n\nv0.9.0\n: Vanchi says \"eat\"\n\n\nAlso, vanchi is so paranoid he's worried immediately about this. \n- mysql 5.6 support (without checksum support, yet)\n- fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)\n\n\nv0.8.1\n: \"Pascal says Bonjour\"\n\n\n\n\nminor bugfix release around mysql connections going away.\n\n\n\n\nv0.8.0\n: the cat never shuts up\n\n\n\n\nadd \"ts\" field to row output\n\n\nadd --config option for passing a different config file\n\n\nsupport int1, int2, int4, int8 columns\n\n\n\n\nv0.7.2\n: \"all the sql ladies\"\n\n\n\n\nhandle inline sql comments\n\n\nignore more user management SQL\n\n\n\n\nv0.7.1\n: \"not hoarders\"\n\n\n\n\nonly keep 5 most recent schemas\n\n\n\n\nv0.7.0\n: 0.7.0, \"alameda\"\n\n\n\n\nhandle CURRENT_TIMESTAMP parsing properly\n\n\nbetter binlog position sync behavior\n\n\n\n\nv0.6.3\n: 0.6.3\n\n\n\n\nbetter blacklist for CREATE TRIGGER\n\n\n\n\nv0.6.2\n: v0.6.2\n\n\n\n\nmaxwell now ignores SAVEPOINT statements.\n\n\n\n\nv0.6.1\n: v0.6.1\n\n\n\n\nfixes a bug with parsing length-limited indexes.\n\n\n\n\nv0.6.0\n: kafkakafkakafa\n\n\nVersion 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy.  It also fixes a couple of bugs in the query parsing path.\n\n\nv0.5.0\n: 0.5.0 -- \"People who put commas in column names deserve undefined behavior\"\n\n\n\n\nmaxwell now captures primary keys on tables.  We'll use this to form kafka key names later.\n\n\nmaxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.\n\n\n\n\nv0.4.0\n: 0.4.0, \"unboxed cat\"\n\n\nv0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.\n\n\nv0.3.0\n: 0.3.0\n\n\nThis version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. \n\n\nIt also enables kafka gzip compression by default.\n\n\nv0.2.2\n: 0.2.2\n\n\nVersion 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. \n\n\nv0.2.1\n: v0.2.1\n\n\nversion 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.\n\n\nv0.2.0\n: 0.2.0\n\n\nThis release gets Maxwell storing the last-written binlog position inside the mysql master itself. \n\n\nv0.1.4\n: 0.1.4\n\n\nsupport --position_file param\n\n\nv0.1.3\n: 0.1.3\n\n\nAdds kafka command line options.\n\n\nv0.1.1\n: 0.1.1\n\n\nv0.1.1, a small bugfix release. \n\n\nv0.1\n: 0.1\n\n\nThis is the first possible release of Maxwell that might work.  It includes some exceedingly basic kafka support, and JSON output of binlog deltas.", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#maxwell-changelog", 
            "text": "", 
            "title": "Maxwell changelog"
        }, 
        {
            "location": "/changelog/#v1221-a-snow-covered-field", 
            "text": "fix crash in rabbit-mq producer  better support for maxwell + azure-mysql  remove bogus different-host bootstrap check  some security upgrades", 
            "title": "v1.22.1: \"a snow covered field\""
        }, 
        {
            "location": "/changelog/#v1220-through-the-roof-and-underground", 
            "text": "Bootstrapping has been reworked and is now available in all setups,\nincluding those in which the maxwell store is split from the replicator.  cleanup and fix a deadlock in the kafka fallback queue logic  add .partition_string = to javascript filters", 
            "title": "v1.22.0: \"through the roof, and underground\""
        }, 
        {
            "location": "/changelog/#v1211-ohhhhhh-oh-oh", 
            "text": "Upgrade binlog connector.  Should fix issues around deserialization\nerrors.", 
            "title": "v1.21.1: \"ohhhhhh oh oh\""
        }, 
        {
            "location": "/changelog/#v1210-through-the-roof", 
            "text": "Bootstrapping output no longer contain binlog positions.  Please update\n  any code that relies on this.  Fix 3 parser issues.", 
            "title": "v1.21.0: \"through the roof\""
        }, 
        {
            "location": "/changelog/#v1200-and-so-you-learn-the-only-way-to-go-is", 
            "text": "add support for partitioning by transaction ID thx @hexene  add support for a kafka \"fallback\" topic to write to\n  when a message fails to write  add UJIS charset support  parser bug: multiple strings concatenate to make one default string  parser bug: deal with bizarre column renames which are then referenced\n  in AFTER column statements", 
            "title": "v1.20.0: \"and so you learn the only way to go is\""
        }, 
        {
            "location": "/changelog/#v1197-in-every-corner-of-your-room", 
            "text": "fix a parser error with empty sql comments  interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei", 
            "title": "v1.19.7: \"in every corner of your room\""
        }, 
        {
            "location": "/changelog/#v1196-set-up-for-you", 
            "text": "Further fixes for GTID-reconnection issues.  Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master,\n  thanks @acampoh", 
            "title": "v1.19.6: \"set up for you\""
        }, 
        {
            "location": "/changelog/#v1195-when-there-is-trap", 
            "text": "Fixes for unreliable connections wrt to GTID events; previously we\n  restart in any old position, now we throw away the current transaction\n  and restart the replicator again at the head of the GTID event.", 
            "title": "v1.19.5: \"when there is trap\""
        }, 
        {
            "location": "/changelog/#v1194-and-underground", 
            "text": "Fixes for a maxwell database not making it through the blacklist  Add  output_null_zerodates  parameter to control how we treat\n  '0000-00-00'", 
            "title": "v1.19.4: \"and underground\""
        }, 
        {
            "location": "/changelog/#v1193-through-the-roof", 
            "text": "Add a universal backpressure mechanism.  This should help people who\nwere running into out-of-memory situations while bootstrapping.", 
            "title": "v1.19.3: \"through the roof\""
        }, 
        {
            "location": "/changelog/#v1192-the-same-i-wore-last-night", 
            "text": "Include schema_id in bootstrap events  add more logging around binlog connector losing connection  add retry logic to redis  some aws fixes   allow pushing JS hashes/arrays into data from js filters    list changes", 
            "title": "v1.19.2: \"the same I wore last night\""
        }, 
        {
            "location": "/changelog/#v1191-the-swoop-here-doesnt-change-things-one-bit", 
            "text": "Handle mysql bit literals in DEFAULT statements  blacklist out CREATE ROLE etc  upgrade dependencies to pick up security issues", 
            "title": "v1.19.1: \"the swoop here doesn't change things one bit\""
        }, 
        {
            "location": "/changelog/#v1190-whole-lotta-milka", 
            "text": "mysql 8 support!  utf8 enum values are supported now  fix #1125, bootstrapping issue for TINYINT(1)  fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\"  only resume bootstraps that are targeted at this client_id  fixes for blacklists and heartbeats.  Did I ever mention blacklists\n  are a terrible idea?", 
            "title": "v1.19.0: \"whole lotta milka\""
        }, 
        {
            "location": "/changelog/#v1180-hello-from-the-andes", 
            "text": "memory optimizations for large schemas (especially shareded schemas with lots of duplicates)  add support for an http endpoint to support Prometheus metrics  allow javascript filters to access the row query object  javascript filters now run in the bootstrap process  support for non-latin1 column names  add  --output_schema_id  option  better handling of packet-too-big errors from Kinesis  add message.publish.age metric", 
            "title": "v1.18.0: \"hello from the Andes\""
        }, 
        {
            "location": "/changelog/#v1171-ay-ay-ay", 
            "text": "fix a regression around filters + bootstrapping  fix a regression around filters + database-only-ddl", 
            "title": "v1.17.1: \"ay, ay, ay\""
        }, 
        {
            "location": "/changelog/#v1170-monday-not-sunday-tuesday", 
            "text": "v1.17.0 brings a new level of configurability by allowing you to inject\na bit of javascript into maxwell's processing.  Should be useful!  Also:   fix regression for Alibaba RDS tables", 
            "title": "v1.17.0: \"monday, not sunday tuesday\""
        }, 
        {
            "location": "/changelog/#v1161-the-90-degree-angle-thing", 
            "text": "Fix Bootstrapping for JSON columns  add --recapture_schema flag for when ya wanna start over  add kafka 1.0 libraries, make them default", 
            "title": "v1.16.1: \"the 90 degree angle thing\""
        }, 
        {
            "location": "/changelog/#v1160-kind-of-sort-of-a-reference-to-something", 
            "text": "v1.16.0 brings a rewrite of Maxwell's filtering system, giving it a\nconcise list of rules that are executed in sequence.  It's now possible\nto exclude tables from a particular database, exclude columns matching a\nvalue, and probably some other use cases.\nSee http://maxwells-daemon.io/config/#filtering for details.", 
            "title": "v1.16.0: \"kind of sort of a reference to something\""
        }, 
        {
            "location": "/changelog/#v1150-im-sure-im-being-supportive-here", 
            "text": "This is a bug-fix release, but it's big enough I'm giving it a minor\nversion.   Fix a very old bug in which DDL rows were writing the  start  of the\nrow into  maxwell.positions , leading to chaos in some scenarios where\nmaxwell managed to stop on the row and double-process it, as well as to\na few well-meaning patches.  Fix the fact that maxwell was outputting \"next-position\" instead of\n\"position\" of a row into JSON.  Fix the master-recovery code to store schema that corresponds to the\nstart of a row, and points the replicator at the next-position.   Much thanks to Tim, Likun and others in sorting this mess out.", 
            "title": "v1.15.0: \"I'm sure I'm being supportive here.\""
        }, 
        {
            "location": "/changelog/#v1147-casamir-pulaski-day", 
            "text": "add RowMap#getRowQuery, thx @saimon7  revert alpine-linux docker image fiasco  fix RawJSONString not serializable, thx @niuhaifeng", 
            "title": "v1.14.7: \"casamir pulaski day\""
        }, 
        {
            "location": "/changelog/#v1146-gimme-one-sec-i-need-to-grab-something", 
            "text": "Fix docker image", 
            "title": "v1.14.6: \"gimme one sec, I need to grab something\""
        }, 
        {
            "location": "/changelog/#v1145-he-looks-funny-he-moves-funny", 
            "text": "reduce docker image footprint  add benchmarking framework  performance improvements for date/datetime columns  fix parser error on UPGRADE PARTITIONING", 
            "title": "v1.14.5: \"he looks funny, he moves funny\""
        }, 
        {
            "location": "/changelog/#v1144-chinese-food", 
            "text": "Fix race condition in SchemaCapturer", 
            "title": "v1.14.4: \"chinese food\""
        }, 
        {
            "location": "/changelog/#v1143-whats-for-lunch", 
            "text": "Enable jvm metrics", 
            "title": "v1.14.3: \"what's for lunch?\""
        }, 
        {
            "location": "/changelog/#v1142-bork-bork-bork", 
            "text": "fix regression in 1.14.1 around bootstrapping host detection  fix heartbeating code around table includes", 
            "title": "v1.14.2: \"bork bork bork\""
        }, 
        {
            "location": "/changelog/#v1141-half-asleep-in-frog-pajamas", 
            "text": "bootstraps can now take a client_id  improved config validation for embedded mode", 
            "title": "v1.14.1: \"half asleep in frog pajamas\""
        }, 
        {
            "location": "/changelog/#v1140-cats-cats-more-cats-sadness-at-lack-of-cats", 
            "text": "new feature  --output_xoffset  to uniquely identify rows within transactions,\n  thx Jens Gyti  Bug fixes around \"0000-00-00\" times.  Bug fixes around dates pre 1000 AD", 
            "title": "v1.14.0: \"cats, cats, more cats.  sadness at lack of cats.\""
        }, 
        {
            "location": "/changelog/#v1135-cyclone-keni-is-real", 
            "text": "Support environment variable based configuration", 
            "title": "v1.13.5: \"cyclone keni is real\""
        }, 
        {
            "location": "/changelog/#v1134-it-was-just-a-dream", 
            "text": "Added possibility to do not declare the rabbitmq exchange.", 
            "title": "v1.13.4: \"it was just a dream\""
        }, 
        {
            "location": "/changelog/#v1133-winner-winner-chicken-dinner", 
            "text": "Add logging for binlog errors  Maven warning fix  Do not include current position DDL schema to avoid processing DDL twice  Always write null fields in primary key fields  Bugfix: fix http_path_prefix command line option issue", 
            "title": "v1.13.3: \"winner winner chicken dinner\""
        }, 
        {
            "location": "/changelog/#v1132-i-just-bought-them-to-sleep-in", 
            "text": "fix a bug with CHARACTER SET = DEFAULT  maxwell now eclipse-friendly.  configurable bind-address for maxwell's http server", 
            "title": "v1.13.2: \"I just bought them to sleep in\""
        }, 
        {
            "location": "/changelog/#v1131-line-up-your-exes-in-song", 
            "text": "redis producer now supports LPUSH, thx @m-denton  RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti  bugfix: fix jdbc option parsing when value contains  =  bugfix: apparently the SQS producer was disabled  bugfix: fix a situation where adding a second client could cause\n  schemas to become out of sync  support for --daemon", 
            "title": "v1.13.1: \"line up your exes in song\""
        }, 
        {
            "location": "/changelog/#v1130-sorry-i-burned-your-clothes", 
            "text": "proper SSL connection support, thanks @cadams5  support for including original SQL in insert/update/deletes, thanks @saimon7  fixes for float4, float8 and other non-mysql datatypes  bump kinesis lib to 0.12.8  fix for bug when two databases share a single table", 
            "title": "v1.13.0: \"sorry, I burned your clothes\""
        }, 
        {
            "location": "/changelog/#v1120-cold-feet-literally-and-metaphorically", 
            "text": "Support for injecting a custom producer, thanks @tomcollinsproject  New producer for Amazon SQS, thanks @vikrant2mahajan  Maxwell can now filter rows based on column values, thanks @finnplay  Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay  DDL output can now optionally include the source SQL, thanks @sungjuly  Support for double-quoted table/database/etc names  rabbitmq option for persistent messages, thanks @d-babiak  SQL parser bugfix for values like +1.234, thanks @hexene", 
            "title": "v1.12.0: \"Cold Feet, literally and metaphorically.\""
        }, 
        {
            "location": "/changelog/#v1110-the-latest-the-greatest", 
            "text": "- default kafka client upgrades to 0.11.0.1\n - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803)", 
            "title": "v1.11.0: \"the latest, the greatest\""
        }, 
        {
            "location": "/changelog/#v1109-no-one-left-behind", 
            "text": "We recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9.   Add missing Kafka clients  Listen and report on binlog connector lifecycle events for better visibility  Reduce docker image size", 
            "title": "v1.10.9: \"no one left behind\""
        }, 
        {
            "location": "/changelog/#v1108-what-doesnt-kill-you-makes-you-stronger", 
            "text": "Fix docker builds  Add Google Cloud Pub/Sub producer  RabbitMQ producer enhancements", 
            "title": "v1.10.8: \"what doesn't kill you makes you stronger\""
        }, 
        {
            "location": "/changelog/#v1107-its-never-too-l8", 
            "text": "Java 8 upgrade  Diagnostic health check endpoint  Encryption  Documentation update: encryption, kinesis producer, schema storage fundamentals, etc.", 
            "title": "v1.10.7: \"it's never too l8!\""
        }, 
        {
            "location": "/changelog/#v1106-a-new-starter-is-here", 
            "text": "Binlog-connector upgrade  Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special  If master recovery is enabled, Maxwell cleans up old positions for the same server and client id", 
            "title": "v1.10.6: \"a new starter is here\""
        }, 
        {
            "location": "/changelog/#v1105-half-asleep-on-her-couch", 
            "text": "Shyko's binlog-connector is now the default and only replication\nbackend available for maxwell.", 
            "title": "v1.10.5: \"half asleep on her couch\""
        }, 
        {
            "location": "/changelog/#v1104-shutdown-harder", 
            "text": "Notable changes:   Shutdown hardening. If maxwell can't shut down (because the kafka\n   producer is in a bad state and  close()  never terminates, for example),\n   it would previously stall and process no messages. Now, shutdown is run\n   in a separate thread and there is an additional watchdog thread which\n   forcibly kills the maxwell process if it can't shut down within 10\n   seconds.  Initial support for running maxwell from java, rather then as its own\n   process. This mode of operation is still experimental, but we'll\n   accept PRs to improve it (thanks Geoff Lywood).  Fix incorrect handling of negative (pre-epoch dates) when using\n   binlog_connector mode (thanks Geoff Lywood).", 
            "title": "v1.10.4: \"shutdown --harder\""
        }, 
        {
            "location": "/changelog/#v1103-1102-and-a-bit", 
            "text": "tiny release to fix a units error in the  replication.lag  metric\n   (subtracting seconds from milliseconds)", 
            "title": "v1.10.3: \"1.10.2-and-a-bit\""
        }, 
        {
            "location": "/changelog/#v1102-just-in-time-for-tomorrow", 
            "text": "added metrics: \"replication.queue.time\" and \"inflightmessages.count\"  renamed \"time.overall\" metric to \"message.publish.time\"  documentation updates (thanks Chintan Tank)", 
            "title": "v1.10.2: \"just in time for tomorrow\""
        }, 
        {
            "location": "/changelog/#v1101-forgive-and-forget", 
            "text": "The observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes:      Support Kafka 0.10.2     Stop procesing RDS hearbeats     Keep maxwell heartbeat going every 10 seconds when database is quiet     Allow for empty double-quoted string literals for database schema changes     Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error", 
            "title": "v1.10.1: \"forgive and forget\""
        }, 
        {
            "location": "/changelog/#v1100-slightly-more-ones-than-zeroes", 
            "text": "This is a small release, primarily around a change to how schemas are\nstored. Maxwell now stores the  last_heartbeat_read  with each entry\nin the  schemas  table, making schema management more resilient to\ncases where binlog numbers are reused, but means that you must take\ncare if you need to roll back to an earlier version. If you deploy\nv1.10.0, then roll back to an earlier version, you should manually\nupdate all  schemas . last_heartbeat_read  values to  0  before\nredeploying v1.10.0 or higher.  Other minor changes:   allow negative default numbers in columns  only store final binlog position if it has changed  blacklist internal aurora table `rds_heartbeat*'  log4j version bump (allows for one entry per line JSON logging)", 
            "title": "v1.10.0: \"slightly more ones than zeroes\""
        }, 
        {
            "location": "/changelog/#v190-now-with-added-whimsy", 
            "text": "Maxwell 1.9 adds one main feature: monitoring support, contributed by\nScott Ferguson. Multiple backends can be configured, read the updated\ndocs for full details.  There's also some bugfixes:   filter DDL messages based on config  determine newest schema from binlog order, not creation order  add task manager to shutdown cleanly on error  minor logging improvements", 
            "title": "v1.9.0: \"now with added whimsy\""
        }, 
        {
            "location": "/changelog/#v182-just-as-the-postcards-wept", 
            "text": "Bugfix release.   maxwell would crash on a quoted partition name  fixes for alters on non-string tables containing VARCHAR  use seconds instead of milliseconds for DDL messages", 
            "title": "v1.8.2: \"just as the postcards wept\""
        }, 
        {
            "location": "/changelog/#v181-famous-is-faster-dont-have-to-be-talented", 
            "text": "performance improves in capturing and restoring schema, thx Joren\n  Minnaert  Allow for capturing from a separate mysql host (adds support for using\n  Maxscale as a replication proxy), thx Adam Szkoda", 
            "title": "v1.8.1: \"famous is faster, don't have to be talented\""
        }, 
        {
            "location": "/changelog/#v180-upbeat-honest-contradictory", 
            "text": "In version 1.8.0 Maxwell gains alpha support for GTID-based positions!\nAll praise due to Henry Cai.", 
            "title": "v1.8.0: \"upbeat, honest, contradictory\""
        }, 
        {
            "location": "/changelog/#v172-comparing-self-to-better", 
            "text": "Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a\n  binlog event, leading to crashes or in some cases data mismatches.", 
            "title": "v1.7.2: \"comparing self to better\""
        }, 
        {
            "location": "/changelog/#v171-blame-it-on-your-seratonin", 
            "text": "bootstrapping now can take a  --where  clause  performance improvements in the kafka producer", 
            "title": "v1.7.1: \"blame it on your seratonin\""
        }, 
        {
            "location": "/changelog/#v170-lucky-me-lucky-mud", 
            "text": "Maxwell 1.7 brings 2 major new, alpha features.  The first is Mysql 5.7\nsupport, including JSON column type support and handling of 5.7 SQL, but not  including GTID support yet.  This is based on porting Maxwell to\nStanley Shyko's binlog-connector library.  Thanks to Stanley for his\namazing support doing this port.  The second major new feature is a producer for Amazon's Kinesis streams,\nThis was contributed in full by the dogged and persistent Thomas Dziedzic.\nCheck it out with  --producer=kinesis .  There's also some bugfixes:\n- Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson\n- allow CHECK() statements inside column definitions", 
            "title": "v1.7.0: \"lucky me, lucky mud\""
        }, 
        {
            "location": "/changelog/#v160-give-me-a-quest", 
            "text": "This is mostly a bugfix release, but it gets a minor version bump due to\na single change of behavior: dates and timestamps which mysql may\naccept, but are considered invalid (0000-00-00 is a notable example)\npreviously had inconsistent behavior.  Now we convert these to NULL.\nOther bugfixes:\n- heartbeats have moved into their own table\n- more fixes around alibaba rds\n- ignore DELETE statements that are output for MEMORY tables upon server\n  restart\n- allow pointing maxwell to a pre-existing database", 
            "title": "v1.6.0: \"give me a quest\""
        }, 
        {
            "location": "/changelog/#v152-french-banana", 
            "text": "add support for kafka 0.10.1 @ smferguson  master recovery: cleanup positions from previous master; prevent\n  errors on flip-back.  fix a bug that would trigger in certain cases when dropping a column\n  that was part of the primary-key", 
            "title": "v1.5.2: \"french banana\""
        }, 
        {
            "location": "/changelog/#v151-151-is-just-151", 
            "text": "This is a bugfix release.\n- fixes for bootstrapping with an alternative maxwell-schema name and an\n   include_database  filter, thanks Lucian Jones\n- fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson\n- ignore the RDS table  mysql.ha_health_check  table\n- Get the bootstrapping process to output NULL values.\n- fix a quoting issue in the bootstrap code, thanks @mylesjao.", 
            "title": "v1.5.1: \"1.5.1 is just 1.5.1\""
        }, 
        {
            "location": "/changelog/#v150-someone-somewhere-is-still-smoking-cigarettes-damnit", 
            "text": "CHANGE: Kafka producer no longer ships with hard-coded defaults.\n  Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\"\n  configured to your liking.  bugfix: fix a regression in handling  ALTER TABLE change c int after b  statements  warn on servers with missing server_id", 
            "title": "v1.5.0: \"someone, somewhere, is still smoking cigarettes, damnit\""
        }, 
        {
            "location": "/changelog/#v142-drawer-cat-is-back", 
            "text": "kafka 0.10.0 support, as well as a re-working of the --kafka_version\n  command line option.", 
            "title": "v1.4.2: \"drawer cat is back\""
        }, 
        {
            "location": "/changelog/#v141-cat-snores", 
            "text": "support per-table topics, Thanks @smferguson and @sschatts.  fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson", 
            "title": "v1.4.1: \"cat snores\""
        }, 
        {
            "location": "/changelog/#v140-deep-insomniac-character-flaws", 
            "text": "1.4.0 brings us two nice new features:\n- partition-by-column: see --kafka_partition_columns.  Thanks @smferguson\n- output schema changes as JSON: see --output_ddl.  Thanks @xmlking\n- As well as a fix around race conditions on shutdown.", 
            "title": "v1.4.0: \"deep, insomniac character flaws\""
        }, 
        {
            "location": "/changelog/#v130-yogg-saron", 
            "text": "support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan  support for outputting server_id   thread_id, thanks @sagiba  fix a race condition in bootstrap support", 
            "title": "v1.3.0: \"yogg-saron\""
        }, 
        {
            "location": "/changelog/#v122-bats-wearing-frog-pajamas", 
            "text": "Maxwell will now include by default fields with NULL values (as null\n  fields).  To disable this and restore the old functionality where fields\n  were omitted, pass  --output_nulls=false  Fix an issue with multi-client support where two replicators would\n  ping-pong heartbeats at each other  Fix an issue where a client would attempt to recover a position from a\n  mismatched client_id  Fix a bug when using CHANGE COLUMN on a primary key", 
            "title": "v1.2.2: \"bats wearing frog pajamas\""
        }, 
        {
            "location": "/changelog/#v121-point-ones-are-a-sad-and-inevitable-fact", 
            "text": "This is a bugfix release.\n- fix a parser bug around ALTER TABLE CHARACTER SET\n- fix bin/maxwell to pull in the proper version of the kafka-clients\n  library", 
            "title": "v1.2.1: \"point-ones are a sad and inevitable fact\""
        }, 
        {
            "location": "/changelog/#v120-just-here-not-to-talk-to-you", 
            "text": "1.2.0 is a major release of Maxwell that introduces master recovery\nfeatures; when a slave is promoted to master, Maxwell is now capable of\nrecovering the position.  See the  --master_recovery  flag for more\ndetails.  It also upgrades the kafka producer library to 0.9.  If you're using\nmaxwell with a kafka 0.8 server, you must now pass the  --kafka0.8  flag\nto maxwell.", 
            "title": "v1.2.0: \"just here, not to talk to you\""
        }, 
        {
            "location": "/changelog/#v116-pithy", 
            "text": "minor bugfix in which maxwell with --replay mode was trying to write\n  heartbeats", 
            "title": "v1.1.6: \"pithy\""
        }, 
        {
            "location": "/changelog/#v115-my-brain-is-a-polluted-mess", 
            "text": "@dadah89 adds --output_binlog_position to optionally output the\n  position with the row  @dadah89 adds --output_commit_info to turn off xid/commit fields  maxwell now supports tables with partitions  maxwell now supports N maxwells per-server.  see the client_id /\n  replica_server_id options.  two parser fixes, for engine= innodb  and CHARSET ASCII  lay the ground work for doing master recovery; we add a heartbeat into\n  the positions table that we can co-ordinate around.", 
            "title": "v1.1.5: \"my brain is a polluted mess\""
        }, 
        {
            "location": "/changelog/#v114-george-flunk", 
            "text": "add support for a bunch more charsets (gbk, big5, notably)  fix Maxwell's handling of kafka errors - previously we were trying to\n  crash Maxwell by throwing a RuntimeException out of the Kafka\n  Producer, but this was a failure.  Now we log and skip all errors.", 
            "title": "v1.1.4: \"george flunk\""
        }, 
        {
            "location": "/changelog/#v113-the-button-i-push-to-not-have-to-go-out", 
            "text": "This is a bugfix release, which fixes:\n- https://github.com/zendesk/maxwell/issues/376, a problem parsing\n  RENAME INDEX\n- https://github.com/zendesk/maxwell/issues/371, a problem with the\n  SERIAL datatype\n- https://github.com/zendesk/maxwell/issues/362, we now preserve the\n  original casing of columns\n- https://github.com/zendesk/maxwell/issues/373, we were incorrectly\n  expecting heartbeats to work under 5.1", 
            "title": "v1.1.3: \"the button I push to not have to go out\""
        }, 
        {
            "location": "/changelog/#v112-scribbled-notes-on-red-pages", 
            "text": "pick up latest mysql-connector-j, fixes #369  fix an issue where maxwell could skip ahead positions if a leader failed.  rework buffering code to be much kinder to the GC and JVM heap in case\n  of very large transactions / rows inside transactions  kinder, gentler help text when you specify an option incorrectly", 
            "title": "v1.1.2: \"scribbled notes on red pages\""
        }, 
        {
            "location": "/changelog/#v111-scribbled-notes-on-blue-pages", 
            "text": "fixes a race condition setting the binlog position that would get\n  maxwell stuck", 
            "title": "v1.1.1: scribbled notes on blue pages"
        }, 
        {
            "location": "/changelog/#v110-sleep-away-the-afternoon", 
            "text": "much more efficient processing of schema updates storage, especially when dealing with large schemas.  @lileeyao added --exclude-columns and the --jdbc_options features  @lileeyao added --jdbc_options  can now blacklist entire databases  new kafka key format available, using a JSON array instead of an object  bugfix: unsigned integer columns were captured incorrectly.  1.1 will\n  recapture the schema and attempt to correct the error.", 
            "title": "v1.1.0: \"sleep away the afternoon\""
        }, 
        {
            "location": "/changelog/#v110-pre4-buck-buck-buck-buck-buck-buck-ah", 
            "text": "Eddie McLean gives some helpful patches around bootstrapping  Bugfixes for the patch-up-the-schema code around unsigned ints", 
            "title": "v1.1.0-pre4: \"buck buck buck buck buck buck-AH!\""
        }, 
        {
            "location": "/changelog/#v110-pre3", 
            "text": "forgot to include some updates that back-patch unsigned column\n  problems", 
            "title": "v1.1.0-pre3:"
        }, 
        {
            "location": "/changelog/#v110-pre2-yawn-yawn", 
            "text": "fix performance issues when capturing schema in AWS Aurora  fix a bug in capturing unsigned integer columns", 
            "title": "v1.1.0-pre2: \"yawn yawn\""
        }, 
        {
            "location": "/changelog/#v101-bag-of-oversized-daisies", 
            "text": "fixes a parsing bug with  CURRENT_TIMESTAMP()", 
            "title": "v1.0.1: \"bag of oversized daisies\""
        }, 
        {
            "location": "/changelog/#v100-maxwell-learns-to-speak", 
            "text": "Since v0.17.0, Maxwell has gotten:\n- bootstrapping support\n- blacklisting for tables\n- flexible kafka partitioning\n- replication heartbeats\n- GEOMETRY columns\n- a whole lotta lotta bugfixes  and I, Osheroff, think the damn thing is stable enough for a 1.0.  So\nthere.", 
            "title": "v1.0.0: \"Maxwell learns to speak\""
        }, 
        {
            "location": "/changelog/#v100-rc3-cmon-and-take-it", 
            "text": "pull in support for replication heartbeats.  helps in the flakier\nnetwork environs.", 
            "title": "v1.0.0-RC3: \"C'mon and take it\""
        }, 
        {
            "location": "/changelog/#v100-rc2-same-thing-just-without-the-v", 
            "text": "fixes the way ALTER DATABASE charset= was handled  adds proper handling of ALTER TABLE CONVERT TO CHARSET", 
            "title": "v1.0.0-RC2: \"same thing, just without the v\""
        }, 
        {
            "location": "/changelog/#v100-rc1-richard-buckners-release", 
            "text": "modifications to the way the bootstrap utility works  fix a race condition crash bug in bootstrapping  fix a parser bug", 
            "title": "v1.0.0-RC1: \"Richard Buckner's release\""
        }, 
        {
            "location": "/changelog/#v100-pre2-an-embarassment-of-riches", 
            "text": "1.0.0-PRE2 brings in a lot of changes that got merged while we were\ntesting out PRE1.  so, hey.\n- Configurable names for the  maxwell  schema database (Kristian Kaufman)\n- Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman)\n- Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman)\n- support GEOMETRY columns, output as well-known-text\n- add  --blacklist_tables  option to fully ignore excessive schema changes (Nicolas Maquet)\n- bootstrap rows now have 'bootstrap-insert' type", 
            "title": "v1.0.0-PRE2: \"an embarassment of riches\""
        }, 
        {
            "location": "/changelog/#v100-pre1-drunk-conversations-with-sober-people", 
            "text": "Here we have the preview release of @nmaquet's excellent work around\n  bootstrapping initial versions of mysql tables.", 
            "title": "v1.0.0-PRE1: \"drunk conversations with sober people\""
        }, 
        {
            "location": "/changelog/#v0170-wrists-of-william", 
            "text": "v0.17 is a large bugfix release with one new feature.\n- FEATURE: allow specifying an alternative mysql schema-storage server and\n  replication server\n- BUGFIX: properly handle case-sensitivity by aping the behavior of the\n  master server.  Fixes #230.\n- BUGFIX: parse some forms of CHECK( ... ) statements.  Fixes #203.\n- BUGFIX: many more SQL-parser fixes.  We are mostly through some\n  thousands of lines of SQL produced by mysql-test.", 
            "title": "v0.17.0: \"wrists of William\""
        }, 
        {
            "location": "/changelog/#v0162-the-best-laid-plans", 
            "text": "This is a large-ish bugfix release.\n- Support, with reservations, binlog_row_image=MINIMAL\n- parser bug: handle table names that look like floating points\n- parser bug: fix for entity names that have '.', '\\', etc in them\n- handle UPPERCASE encoding names\n- support UCS2 (start trying to operate ok on the mysql-test suite)\n- use ObjectOutputStream.reset to fix memory leaks when buffering to disk", 
            "title": "v0.16.2: \"The best laid plans\""
        }, 
        {
            "location": "/changelog/#v0161-me-and-room-service", 
            "text": "This is a bug-fix-roundup release:\n- support ALTER DATABASE\n- fix a bunch of parse errors: we've started running mysql-test at\n  maxwell and are fixing up failures.\n- some modifications to the overflow-to-disk logic; we buffer the input\n  and output, and we fix a memory leak", 
            "title": "v0.16.1: \"me and room service\""
        }, 
        {
            "location": "/changelog/#v0160-kristian-kaufmanns-version", 
            "text": "Version 0.16.0 introduces a feature where UPDATE statements will now\nshow both the new row image and the old values of the fields that\nchanged.  Thanks @kristiankaufmann", 
            "title": "v0.16.0: \"Kristian Kaufmann's version\""
        }, 
        {
            "location": "/changelog/#v0150-the-littlest-little-city", 
            "text": "fix a parse problem with indices ordered by ASC/DESC", 
            "title": "v0.15.0: \"the littlest little city\""
        }, 
        {
            "location": "/changelog/#v0150-rc1-its-later-than-you-think", 
            "text": "large transactions now buffer to disk instead of crushing maxwell.  support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters", 
            "title": "v0.15.0-RC1: \"it's later than you think\""
        }, 
        {
            "location": "/changelog/#v0146-its-about-being-american-sort-of", 
            "text": "fix TIME column support  fix parsing on millisecond precision column defintions  fix CREATE SCHEMA parsing", 
            "title": "v0.14.6: \"It's about being American.  Sort of.\""
        }, 
        {
            "location": "/changelog/#v0145-false-is-the-new-true", 
            "text": "handle BOOLEAN columns with true/false defaults", 
            "title": "v0.14.5: \"false is the new true\""
        }, 
        {
            "location": "/changelog/#v0144-youd-think-wed-be-at-10-by-now-wouldnt-you", 
            "text": "fixes parsing of \"mysql comments\" ( /*! .. */ )  More performance improvements, another 10% in a tight loop.", 
            "title": "v0.14.4: \"You'd think we'd be at 1.0 by now, wouldn't you?\""
        }, 
        {
            "location": "/changelog/#v0143-peanuts-my-girlfriend-thinks-about-peanuts", 
            "text": "fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.", 
            "title": "v0.14.3: \"Peanuts.  My girlfriend thinks about peanuts.\""
        }, 
        {
            "location": "/changelog/#v0142-maxwell-sandvik-88", 
            "text": "capture the mysql database along with the rest of the schema.  Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed.", 
            "title": "v0.14.2: \"Maxwell Sandvik 88\""
        }, 
        {
            "location": "/changelog/#v0141-be-liberal-in-what-you-accept-even-if-nonsensical", 
            "text": "fixes a parser bug around named PRIMARY KEYs.", 
            "title": "v0.14.1: \"be liberal in what you accept.  Even if nonsensical.\""
        }, 
        {
            "location": "/changelog/#v0140-the-slow-but-inevitable-slide", 
            "text": "This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.", 
            "title": "v0.14.0: \"the slow but inevitable slide\""
        }, 
        {
            "location": "/changelog/#v0131-well-that-was-somewhat-expected", 
            "text": "v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated.   v0.13.0 contains:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around  CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.", 
            "title": "v0.13.1: \"well that was somewhat expected\""
        }, 
        {
            "location": "/changelog/#v0130-malkovich-malkovich-malkovich-sheldon", 
            "text": "Lucky release number 13 brings some reasonably big changes:\n- Big performance boost for maxwell: 75% faster in some benchmarks\n- @davidsheldon contributed some nice bug fixes around  CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema.\n- we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.  This release has a pretty bad bug.  do not use.", 
            "title": "v0.13.0: \"Malkovich Malkovich Malkovich Sheldon?\""
        }, 
        {
            "location": "/changelog/#v0120-what-do-i-call-them-slippers-why-are-you-jealous", 
            "text": "add support for BIT columns.", 
            "title": "v0.12.0: \"what do I call them?  Slippers?  Why, are you jealous?\""
        }, 
        {
            "location": "/changelog/#v0114-13-steps", 
            "text": "this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event.  I really need to fix this at a lower level, ie the open-replicator level.", 
            "title": "v0.11.4: \"13 steps\""
        }, 
        {
            "location": "/changelog/#v0113-and-the-other-half-is-to-take-the-bugs-out", 
            "text": "this is a bugfix release:\n- fix problems with table creation options inside alter statements (  ALTER TABLE foo auto_increment=10  )\n- fix a host of shutdown-procedure bugs  the test suite should also be way more reliable, not like you care.", 
            "title": "v0.11.3: \".. and the other half is to take the bugs out\""
        }, 
        {
            "location": "/changelog/#v0112-savage-acts-of-unprovoked-violence-are-bad", 
            "text": "This is a bugfix release.  It includes:\n- soft deletions of maxwell.schemas to fix A- B- A master swapping without creating intense replication delay\n- detect and fail early if we see  binlog_row_image=minimal \n- kill off maxwell if the position thread dies\n- fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)", 
            "title": "v0.11.2: \"savage acts of unprovoked violence are bad\""
        }, 
        {
            "location": "/changelog/#v0111-dog-snoring-loudly", 
            "text": "maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid.  this should prevent the worst of cases.", 
            "title": "v0.11.1: \"dog snoring loudly\""
        }, 
        {
            "location": "/changelog/#v0110-cat-waving-gently", 
            "text": "This release of Maxwell preserves transaction information in the kafka stream by adding a  xid  key in the JSON object, as well as a  commit  key for the final row inside the transaction.  It also contains a bugfix around server_id handling.", 
            "title": "v0.11.0: \"cat waving gently\""
        }, 
        {
            "location": "/changelog/#v0101-all-64-of-your-bases-belong-to-shut-up-internet-parrot", 
            "text": "proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded)  fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash  make table option parsing more lenient", 
            "title": "v0.10.1: \"all 64 of your bases belong to... shut up, internet parrot.\""
        }, 
        {
            "location": "/changelog/#v0110-rc1-goin-faster-than-a-rollercoaster", 
            "text": "merge master fixes", 
            "title": "v0.11.0-RC1: \"goin' faster than a rollercoaster\""
        }, 
        {
            "location": "/changelog/#v0100-the-first-word-is-french", 
            "text": "Mysql 5.6 checksum support!  some more bugfixes with the SQL parser", 
            "title": "v0.10.0: \"The first word is French\""
        }, 
        {
            "location": "/changelog/#v0110-pre4-except-for-that-other-thing", 
            "text": "bugfix on v0.11.0-PRE3", 
            "title": "v0.11.0-PRE4: \"except for that other thing\""
        }, 
        {
            "location": "/changelog/#v0110-pre3-nothing-like-a-good-nights-sleep", 
            "text": "handle SAVEPOINT within transactions  downgrade unhandled SQL to a warning", 
            "title": "v0.11.0-PRE3: \"nothing like a good night's sleep\""
        }, 
        {
            "location": "/changelog/#v0110-pre2-you-really-need-to-name-a-pre-release-something-cutesy", 
            "text": "fixes for myISAM \"transactions\"", 
            "title": "v0.11.0-PRE2: \"you really need to name a PRE release something cutesy?\""
        }, 
        {
            "location": "/changelog/#v0110-pre1-a-slow-traffic-jam-towards-the-void", 
            "text": "fix a server_id bug (was always 1 in maxwell.schemas)  JSON output now includes transaction IDs", 
            "title": "v0.11.0-PRE1: \"A slow traffic jam towards the void\""
        }, 
        {
            "location": "/changelog/#v0100-rc4-inspiring-confidence", 
            "text": "deal with BINARY flag in string column creation.", 
            "title": "v0.10.0-RC4: \"Inspiring confidence\""
        }, 
        {
            "location": "/changelog/#v095-long-story-short-thats-why-im-late", 
            "text": "handle the BINARY flag in column creation", 
            "title": "v0.9.5: \"Long story short, that's why I'm late\""
        }, 
        {
            "location": "/changelog/#v0100-rc3-except-for-that-one-thing", 
            "text": "handle \"TRUNCATE [TABLE_NAME]\" statements", 
            "title": "v0.10.0-RC3: \"Except for that one thing\""
        }, 
        {
            "location": "/changelog/#v0100-rc2-rc2-is-always-a-good-sign", 
            "text": "fixes a bug with checksum processing.", 
            "title": "v0.10.0-RC2: \"RC2 is always a good sign.\""
        }, 
        {
            "location": "/changelog/#v0100-rc1-verify-all-the-things", 
            "text": "upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.", 
            "title": "v0.10.0-RC1: \"verify all the things\""
        }, 
        {
            "location": "/changelog/#v094-weve-been-here-before", 
            "text": "allow a configurable number (including unlimited) of schemas to be stored", 
            "title": "v0.9.4: \"we've been here before\""
        }, 
        {
            "location": "/changelog/#v093-some-days-its-just-better-to-stay-in-bed", 
            "text": "bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes", 
            "title": "v0.9.3: \"some days it's just better to stay in bed\""
        }, 
        {
            "location": "/changelog/#v092-cats-tongue", 
            "text": "bump open-replicator buffer to 50mb by default  log to STDERR, not STDOUT   --output_file  option for file producer", 
            "title": "v0.9.2: \"Cat's tongue\""
        }, 
        {
            "location": "/changelog/#v091-bugs-bugs-bugs-lies-statistics", 
            "text": "Maxwell is now aware that column names are case-insenstive  fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.", 
            "title": "v0.9.1: \"bugs, bugs, bugs, lies, statistics\""
        }, 
        {
            "location": "/changelog/#v090-vanchi-says-eat", 
            "text": "Also, vanchi is so paranoid he's worried immediately about this. \n- mysql 5.6 support (without checksum support, yet)\n- fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)", 
            "title": "v0.9.0: Vanchi says \"eat\""
        }, 
        {
            "location": "/changelog/#v081-pascal-says-bonjour", 
            "text": "minor bugfix release around mysql connections going away.", 
            "title": "v0.8.1: \"Pascal says Bonjour\""
        }, 
        {
            "location": "/changelog/#v080-the-cat-never-shuts-up", 
            "text": "add \"ts\" field to row output  add --config option for passing a different config file  support int1, int2, int4, int8 columns", 
            "title": "v0.8.0: the cat never shuts up"
        }, 
        {
            "location": "/changelog/#v072-all-the-sql-ladies", 
            "text": "handle inline sql comments  ignore more user management SQL", 
            "title": "v0.7.2: \"all the sql ladies\""
        }, 
        {
            "location": "/changelog/#v071-not-hoarders", 
            "text": "only keep 5 most recent schemas", 
            "title": "v0.7.1: \"not hoarders\""
        }, 
        {
            "location": "/changelog/#v070-070-alameda", 
            "text": "handle CURRENT_TIMESTAMP parsing properly  better binlog position sync behavior", 
            "title": "v0.7.0: 0.7.0, \"alameda\""
        }, 
        {
            "location": "/changelog/#v063-063", 
            "text": "better blacklist for CREATE TRIGGER", 
            "title": "v0.6.3: 0.6.3"
        }, 
        {
            "location": "/changelog/#v062-v062", 
            "text": "maxwell now ignores SAVEPOINT statements.", 
            "title": "v0.6.2: v0.6.2"
        }, 
        {
            "location": "/changelog/#v061-v061", 
            "text": "fixes a bug with parsing length-limited indexes.", 
            "title": "v0.6.1: v0.6.1"
        }, 
        {
            "location": "/changelog/#v060-kafkakafkakafa", 
            "text": "Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy.  It also fixes a couple of bugs in the query parsing path.", 
            "title": "v0.6.0: kafkakafkakafa"
        }, 
        {
            "location": "/changelog/#v050-050-people-who-put-commas-in-column-names-deserve-undefined-behavior", 
            "text": "maxwell now captures primary keys on tables.  We'll use this to form kafka key names later.  maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.", 
            "title": "v0.5.0: 0.5.0 -- \"People who put commas in column names deserve undefined behavior\""
        }, 
        {
            "location": "/changelog/#v040-040-unboxed-cat", 
            "text": "v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.", 
            "title": "v0.4.0: 0.4.0, \"unboxed cat\""
        }, 
        {
            "location": "/changelog/#v030-030", 
            "text": "This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes.   It also enables kafka gzip compression by default.", 
            "title": "v0.3.0: 0.3.0"
        }, 
        {
            "location": "/changelog/#v022-022", 
            "text": "Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling.", 
            "title": "v0.2.2: 0.2.2"
        }, 
        {
            "location": "/changelog/#v021-v021", 
            "text": "version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.", 
            "title": "v0.2.1: v0.2.1"
        }, 
        {
            "location": "/changelog/#v020-020", 
            "text": "This release gets Maxwell storing the last-written binlog position inside the mysql master itself.", 
            "title": "v0.2.0: 0.2.0"
        }, 
        {
            "location": "/changelog/#v014-014", 
            "text": "support --position_file param", 
            "title": "v0.1.4: 0.1.4"
        }, 
        {
            "location": "/changelog/#v013-013", 
            "text": "Adds kafka command line options.", 
            "title": "v0.1.3: 0.1.3"
        }, 
        {
            "location": "/changelog/#v011-011", 
            "text": "v0.1.1, a small bugfix release.", 
            "title": "v0.1.1: 0.1.1"
        }, 
        {
            "location": "/changelog/#v01-01", 
            "text": "This is the first possible release of Maxwell that might work.  It includes some exceedingly basic kafka support, and JSON output of binlog deltas.", 
            "title": "v0.1: 0.1"
        }
    ]
}